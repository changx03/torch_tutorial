{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground for PyTorch Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from a CSV file using Pandas\n",
    "Download link: https://archive.ics.uci.edu/ml/datasets/Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth        class\n",
       "0          5.1         3.5          1.4         0.2  Iris-setosa\n",
       "1          4.9         3.0          1.4         0.2  Iris-setosa\n",
       "2          4.7         3.2          1.3         0.2  Iris-setosa\n",
       "3          4.6         3.1          1.5         0.2  Iris-setosa\n",
       "4          5.0         3.6          1.4         0.2  Iris-setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data from CSV file\n",
    "file_path = os.path.join('data', 'iris.data')\n",
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    header=None,\n",
    "    names=['SepalLength', 'SepalWidth',\n",
    "           'PetalLength', 'PetalWidth', 'class'],\n",
    "    dtype={'SepalLength': np.float32,\n",
    "           'SepalWidth': np.float32,\n",
    "           'PetalLength': np.float32,\n",
    "           'PetalWidth': np.float32,\n",
    "           'class': np.str},\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Categorical Attribute into Integer Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Iris-setosa\n",
       "1         Iris-setosa\n",
       "2         Iris-setosa\n",
       "3         Iris-setosa\n",
       "4         Iris-setosa\n",
       "            ...      \n",
       "145    Iris-virginica\n",
       "146    Iris-virginica\n",
       "147    Iris-virginica\n",
       "148    Iris-virginica\n",
       "149    Iris-virginica\n",
       "Name: class, Length: 150, dtype: category\n",
       "Categories (3, object): [Iris-setosa, Iris-versicolor, Iris-virginica]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth  class\n",
       "0          5.1         3.5          1.4         0.2      0\n",
       "1          4.9         3.0          1.4         0.2      0\n",
       "2          4.7         3.2          1.3         0.2      0\n",
       "3          4.6         3.1          1.5         0.2      0\n",
       "4          5.0         3.6          1.4         0.2      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'] = df['class'].astype('category')\n",
    "df['class'] = df['class'].cat.codes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data into Train Set and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>7.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SepalLength  SepalWidth  PetalLength  PetalWidth  class\n",
       "126          6.2         2.8          4.8         1.8      2\n",
       "105          7.6         3.0          6.6         2.1      2\n",
       "32           5.2         4.1          1.5         0.1      0\n",
       "146          6.3         2.5          5.0         1.9      2\n",
       "76           6.8         2.8          4.8         1.4      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle data\n",
    "n = len(df.index)\n",
    "shuffled_indices = np.random.permutation(n)\n",
    "df = df.iloc[shuffled_indices]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4) (90,)\n",
      "(60, 4) (60,)\n",
      "float32 float32\n"
     ]
    }
   ],
   "source": [
    "# Split train/test sets\n",
    "n = len(df.index)\n",
    "num_train = int(n * 0.6)\n",
    "num_test = n - num_train\n",
    "x_train = df.iloc[:num_train, :4].values\n",
    "y_train = df.iloc[:num_train, -1].values.astype(np.long)\n",
    "x_test = df.iloc[-num_test:, :4].values\n",
    "y_test = df.iloc[-num_test:, -1].values.astype(np.long)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "print(x_train.dtype, x_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Numpy Array to PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpDataset(Dataset):\n",
    "    \"\"\"Convert Numpy array into PyTorch Dataset\"\"\"\n",
    "    def __init__(self, data, label):\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.label = torch.from_numpy(label)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 4]) torch.Size([60])\n"
     ]
    }
   ],
   "source": [
    "# Prepare DataLoader for PyTorch\n",
    "train_dataset = NpDataset(x_train, y_train)\n",
    "test_dataset = NpDataset(x_test, y_test)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    ")\n",
    "x, y = next(iter(test_dataloader))\n",
    "print(x.size(), y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisNN, self).__init__()\n",
    "        self.fn1 = nn.Linear(4, 6)\n",
    "        self.fn2 = nn.Linear(6, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fn1(x))\n",
    "        x = self.fn2(x)\n",
    "#         x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "        \n",
    "model = IrisNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0681, -1.3097,  1.5607],\n",
       "        [ 0.8679, -1.1022,  1.3850]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model(x[:2])\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IrisNN(\n",
       "  (fn1): Linear(in_features=4, out_features=6, bias=True)\n",
       "  (fn2): Linear(in_features=6, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.NLLLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation for Updating the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_reg(params):\n",
    "    l2 = torch.tensor(0.).to(device)\n",
    "    for param in params:\n",
    "        l2 += param.norm()\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    for x, y in train_dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        n = x.size(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        score = model(x)\n",
    "        loss = loss_fn(score, y) + 0.01*l2_reg(model.parameters())\n",
    "#         loss = loss_fn(score, y)\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = score.max(1, keepdim=True)[1]\n",
    "        num_correct = predictions.eq(y.view_as(predictions)).sum().item()\n",
    "    \n",
    "    acc = num_correct / n\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing Forward for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            n = x.size(0)\n",
    "            \n",
    "            score = model(x)\n",
    "            loss = loss_fn(score, y)\n",
    "            predictions = score.max(1, keepdim=True)[1]\n",
    "            num_correct = predictions.eq(y.view_as(predictions)).sum().item()\n",
    "    \n",
    "    acc = num_correct / n\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100] Train loss: 1.6782 acc: 28.89 - Test loss: 1.3612 acc: 40.00\n",
      "[1/100] Train loss: 1.6101 acc: 28.89 - Test loss: 1.2876 acc: 40.00\n",
      "[2/100] Train loss: 1.4964 acc: 28.89 - Test loss: 1.2096 acc: 40.00\n",
      "[3/100] Train loss: 1.3671 acc: 26.67 - Test loss: 1.1521 acc: 15.00\n",
      "[4/100] Train loss: 1.2534 acc: 13.33 - Test loss: 1.1364 acc: 1.67\n",
      "[5/100] Train loss: 1.1810 acc: 0.00 - Test loss: 1.1701 acc: 28.33\n",
      "[6/100] Train loss: 1.1622 acc: 36.67 - Test loss: 1.2391 acc: 28.33\n",
      "[7/100] Train loss: 1.1871 acc: 36.67 - Test loss: 1.3121 acc: 28.33\n",
      "[8/100] Train loss: 1.2287 acc: 36.67 - Test loss: 1.3588 acc: 28.33\n",
      "[9/100] Train loss: 1.2593 acc: 36.67 - Test loss: 1.3640 acc: 28.33\n",
      "[10/100] Train loss: 1.2638 acc: 36.67 - Test loss: 1.3302 acc: 28.33\n",
      "[11/100] Train loss: 1.2426 acc: 36.67 - Test loss: 1.2726 acc: 28.33\n",
      "[12/100] Train loss: 1.2063 acc: 36.67 - Test loss: 1.2089 acc: 28.33\n",
      "[13/100] Train loss: 1.1686 acc: 36.67 - Test loss: 1.1528 acc: 28.33\n",
      "[14/100] Train loss: 1.1387 acc: 36.67 - Test loss: 1.1102 acc: 28.33\n",
      "[15/100] Train loss: 1.1200 acc: 36.67 - Test loss: 1.0810 acc: 28.33\n",
      "[16/100] Train loss: 1.1110 acc: 36.67 - Test loss: 1.0621 acc: 40.00\n",
      "[17/100] Train loss: 1.1083 acc: 28.89 - Test loss: 1.0501 acc: 40.00\n",
      "[18/100] Train loss: 1.1085 acc: 28.89 - Test loss: 1.0423 acc: 40.00\n",
      "[19/100] Train loss: 1.1093 acc: 28.89 - Test loss: 1.0365 acc: 40.00\n",
      "[20/100] Train loss: 1.1095 acc: 28.89 - Test loss: 1.0318 acc: 40.00\n",
      "[21/100] Train loss: 1.1081 acc: 28.89 - Test loss: 1.0278 acc: 40.00\n",
      "[22/100] Train loss: 1.1053 acc: 28.89 - Test loss: 1.0238 acc: 40.00\n",
      "[23/100] Train loss: 1.1010 acc: 28.89 - Test loss: 1.0199 acc: 43.33\n",
      "[24/100] Train loss: 1.0953 acc: 30.00 - Test loss: 1.0157 acc: 60.00\n",
      "[25/100] Train loss: 1.0882 acc: 54.44 - Test loss: 1.0111 acc: 71.67\n",
      "[26/100] Train loss: 1.0801 acc: 63.33 - Test loss: 1.0062 acc: 71.67\n",
      "[27/100] Train loss: 1.0711 acc: 63.33 - Test loss: 1.0010 acc: 71.67\n",
      "[28/100] Train loss: 1.0613 acc: 63.33 - Test loss: 0.9956 acc: 71.67\n",
      "[29/100] Train loss: 1.0509 acc: 63.33 - Test loss: 0.9898 acc: 71.67\n",
      "[30/100] Train loss: 1.0400 acc: 63.33 - Test loss: 0.9837 acc: 70.00\n",
      "[31/100] Train loss: 1.0286 acc: 64.44 - Test loss: 0.9767 acc: 75.00\n",
      "[32/100] Train loss: 1.0166 acc: 65.56 - Test loss: 0.9688 acc: 73.33\n",
      "[33/100] Train loss: 1.0040 acc: 77.78 - Test loss: 0.9597 acc: 75.00\n",
      "[34/100] Train loss: 0.9908 acc: 83.33 - Test loss: 0.9495 acc: 68.33\n",
      "[35/100] Train loss: 0.9768 acc: 78.89 - Test loss: 0.9380 acc: 68.33\n",
      "[36/100] Train loss: 0.9623 acc: 77.78 - Test loss: 0.9252 acc: 65.00\n",
      "[37/100] Train loss: 0.9471 acc: 75.56 - Test loss: 0.9114 acc: 61.67\n",
      "[38/100] Train loss: 0.9313 acc: 74.44 - Test loss: 0.8964 acc: 61.67\n",
      "[39/100] Train loss: 0.9151 acc: 73.33 - Test loss: 0.8804 acc: 61.67\n",
      "[40/100] Train loss: 0.8983 acc: 72.22 - Test loss: 0.8634 acc: 61.67\n",
      "[41/100] Train loss: 0.8809 acc: 72.22 - Test loss: 0.8451 acc: 61.67\n",
      "[42/100] Train loss: 0.8626 acc: 73.33 - Test loss: 0.8256 acc: 61.67\n",
      "[43/100] Train loss: 0.8436 acc: 74.44 - Test loss: 0.8053 acc: 65.00\n",
      "[44/100] Train loss: 0.8241 acc: 74.44 - Test loss: 0.7845 acc: 68.33\n",
      "[45/100] Train loss: 0.8040 acc: 75.56 - Test loss: 0.7632 acc: 68.33\n",
      "[46/100] Train loss: 0.7837 acc: 80.00 - Test loss: 0.7417 acc: 71.67\n",
      "[47/100] Train loss: 0.7633 acc: 80.00 - Test loss: 0.7205 acc: 71.67\n",
      "[48/100] Train loss: 0.7432 acc: 82.22 - Test loss: 0.6996 acc: 76.67\n",
      "[49/100] Train loss: 0.7234 acc: 83.33 - Test loss: 0.6793 acc: 80.00\n",
      "[50/100] Train loss: 0.7040 acc: 86.67 - Test loss: 0.6597 acc: 81.67\n",
      "[51/100] Train loss: 0.6850 acc: 90.00 - Test loss: 0.6408 acc: 83.33\n",
      "[52/100] Train loss: 0.6665 acc: 92.22 - Test loss: 0.6226 acc: 86.67\n",
      "[53/100] Train loss: 0.6486 acc: 94.44 - Test loss: 0.6050 acc: 90.00\n",
      "[54/100] Train loss: 0.6315 acc: 97.78 - Test loss: 0.5879 acc: 90.00\n",
      "[55/100] Train loss: 0.6151 acc: 98.89 - Test loss: 0.5717 acc: 90.00\n",
      "[56/100] Train loss: 0.5996 acc: 98.89 - Test loss: 0.5562 acc: 91.67\n",
      "[57/100] Train loss: 0.5848 acc: 97.78 - Test loss: 0.5417 acc: 93.33\n",
      "[58/100] Train loss: 0.5710 acc: 96.67 - Test loss: 0.5284 acc: 93.33\n",
      "[59/100] Train loss: 0.5582 acc: 96.67 - Test loss: 0.5163 acc: 91.67\n",
      "[60/100] Train loss: 0.5462 acc: 96.67 - Test loss: 0.5055 acc: 91.67\n",
      "[61/100] Train loss: 0.5348 acc: 96.67 - Test loss: 0.4958 acc: 91.67\n",
      "[62/100] Train loss: 0.5238 acc: 96.67 - Test loss: 0.4873 acc: 93.33\n",
      "[63/100] Train loss: 0.5133 acc: 96.67 - Test loss: 0.4798 acc: 93.33\n",
      "[64/100] Train loss: 0.5033 acc: 97.78 - Test loss: 0.4731 acc: 91.67\n",
      "[65/100] Train loss: 0.4940 acc: 98.89 - Test loss: 0.4667 acc: 91.67\n",
      "[66/100] Train loss: 0.4852 acc: 98.89 - Test loss: 0.4605 acc: 91.67\n",
      "[67/100] Train loss: 0.4769 acc: 98.89 - Test loss: 0.4543 acc: 93.33\n",
      "[68/100] Train loss: 0.4689 acc: 98.89 - Test loss: 0.4481 acc: 93.33\n",
      "[69/100] Train loss: 0.4613 acc: 98.89 - Test loss: 0.4418 acc: 93.33\n",
      "[70/100] Train loss: 0.4540 acc: 98.89 - Test loss: 0.4355 acc: 93.33\n",
      "[71/100] Train loss: 0.4470 acc: 98.89 - Test loss: 0.4291 acc: 93.33\n",
      "[72/100] Train loss: 0.4402 acc: 98.89 - Test loss: 0.4230 acc: 93.33\n",
      "[73/100] Train loss: 0.4338 acc: 98.89 - Test loss: 0.4171 acc: 93.33\n",
      "[74/100] Train loss: 0.4275 acc: 98.89 - Test loss: 0.4115 acc: 93.33\n",
      "[75/100] Train loss: 0.4215 acc: 98.89 - Test loss: 0.4063 acc: 93.33\n",
      "[76/100] Train loss: 0.4156 acc: 98.89 - Test loss: 0.4016 acc: 93.33\n",
      "[77/100] Train loss: 0.4099 acc: 98.89 - Test loss: 0.3972 acc: 93.33\n",
      "[78/100] Train loss: 0.4043 acc: 98.89 - Test loss: 0.3930 acc: 93.33\n",
      "[79/100] Train loss: 0.3989 acc: 100.00 - Test loss: 0.3890 acc: 93.33\n",
      "[80/100] Train loss: 0.3936 acc: 100.00 - Test loss: 0.3849 acc: 91.67\n",
      "[81/100] Train loss: 0.3884 acc: 100.00 - Test loss: 0.3805 acc: 91.67\n",
      "[82/100] Train loss: 0.3834 acc: 100.00 - Test loss: 0.3759 acc: 91.67\n",
      "[83/100] Train loss: 0.3784 acc: 100.00 - Test loss: 0.3712 acc: 93.33\n",
      "[84/100] Train loss: 0.3736 acc: 100.00 - Test loss: 0.3665 acc: 93.33\n",
      "[85/100] Train loss: 0.3688 acc: 100.00 - Test loss: 0.3617 acc: 93.33\n",
      "[86/100] Train loss: 0.3642 acc: 98.89 - Test loss: 0.3572 acc: 93.33\n",
      "[87/100] Train loss: 0.3596 acc: 98.89 - Test loss: 0.3529 acc: 95.00\n",
      "[88/100] Train loss: 0.3551 acc: 98.89 - Test loss: 0.3488 acc: 95.00\n",
      "[89/100] Train loss: 0.3507 acc: 98.89 - Test loss: 0.3451 acc: 95.00\n",
      "[90/100] Train loss: 0.3464 acc: 98.89 - Test loss: 0.3415 acc: 95.00\n",
      "[91/100] Train loss: 0.3422 acc: 98.89 - Test loss: 0.3381 acc: 93.33\n",
      "[92/100] Train loss: 0.3380 acc: 98.89 - Test loss: 0.3348 acc: 93.33\n",
      "[93/100] Train loss: 0.3339 acc: 98.89 - Test loss: 0.3314 acc: 93.33\n",
      "[94/100] Train loss: 0.3300 acc: 98.89 - Test loss: 0.3280 acc: 93.33\n",
      "[95/100] Train loss: 0.3261 acc: 98.89 - Test loss: 0.3244 acc: 93.33\n",
      "[96/100] Train loss: 0.3222 acc: 98.89 - Test loss: 0.3207 acc: 93.33\n",
      "[97/100] Train loss: 0.3185 acc: 98.89 - Test loss: 0.3170 acc: 95.00\n",
      "[98/100] Train loss: 0.3148 acc: 98.89 - Test loss: 0.3133 acc: 95.00\n",
      "[99/100] Train loss: 0.3112 acc: 98.89 - Test loss: 0.3097 acc: 96.67\n"
     ]
    }
   ],
   "source": [
    "seed = 4096\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "            \n",
    "max_epochs = 100\n",
    "for epoch in range(max_epochs):\n",
    "    tr_loss, tr_acc = train()\n",
    "    eva_loss, eva_acc = evaluate()\n",
    "    print(f'[{epoch}/{max_epochs}] Train loss: {tr_loss:.4f} acc: {tr_acc*100:.2f} - Test loss: {eva_loss:.4f} acc: {eva_acc*100:.2f}')\n",
    "    \n",
    "# [0/100] Train loss: 1.3462 acc: 27.78 - Test loss: 1.2278 acc: 41.67    \n",
    "# [99/100] Train loss: 0.2368 acc: 97.78 - Test loss: 0.2155 acc: 96.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Initial Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0986122886681098"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expect_loss = np.log(3)\n",
    "expect_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print parameters\n",
    "Don't do it on large nerual network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 6.4940e-01,  1.6931e-01, -5.1571e-01, -5.1580e-01],\n",
      "        [ 4.6394e-01,  7.8318e-01, -5.5839e-01, -8.0084e-01],\n",
      "        [-2.8041e-01, -4.3104e-01, -1.5579e-02, -4.1927e-02],\n",
      "        [-4.4076e-01,  8.7070e-02,  3.8718e-01, -1.3657e-01],\n",
      "        [-3.8608e-01, -4.5914e-01,  4.0574e-01,  2.9218e-01],\n",
      "        [ 7.4699e-02, -3.2551e-01,  1.1816e+00,  9.3453e-04]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2536,  0.0536, -0.3241, -0.1278,  0.3421, -0.0803], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4034,  0.6951,  0.1387,  0.2628,  0.0015, -0.7449],\n",
      "        [ 0.1327, -0.1364, -0.3493,  0.3160,  0.2932,  0.4585],\n",
      "        [-0.7156, -1.0724,  0.3656, -0.1596, -0.2651,  0.7777]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1948, -0.1498, -0.0759], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p)\n",
    "\n",
    "# Parameter containing:\n",
    "# tensor([[-0.3159, -0.4066, -0.2228,  0.3912],\n",
    "#         [-0.4637, -0.0598,  0.1091, -0.4652],\n",
    "#         [-0.1581, -0.4901, -0.4268, -0.3081],\n",
    "#         [ 0.6481,  0.5173, -0.7721, -0.1273],\n",
    "#         [ 0.6848,  0.0882, -0.2875, -0.6507],\n",
    "#         [ 0.0285, -0.5087,  1.1340,  0.8644]], device='cuda:0',\n",
    "#        requires_grad=True)\n",
    "# Parameter containing:\n",
    "# tensor([ 0.4440, -0.0441, -0.3559,  0.4312,  0.0881, -0.2388], device='cuda:0',\n",
    "#        requires_grad=True)\n",
    "# Parameter containing:\n",
    "# tensor([[ 0.2153,  0.3459, -0.1325,  0.8138,  0.3868, -0.9468],\n",
    "#         [ 0.3085,  0.2449, -0.1748,  0.1355,  0.2751,  0.4146],\n",
    "#         [-0.0276,  0.2333, -0.1011, -0.8747, -0.6208,  1.1263]],\n",
    "#        device='cuda:0', requires_grad=True)\n",
    "# Parameter containing:\n",
    "# tensor([ 0.3609, -0.1801, -0.1377], device='cuda:0', requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delving into Loss Functions for Classification Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the toy example from Lecture 3 - Loss Functions and Optimization from Stanford University on YouTube  \n",
    "Link: https://youtu.be/h7iBpEHGVNc?t=502\n",
    "\n",
    "\n",
    "- Suppose: 3 training examples within 3 classes\n",
    "- 1 cat, 1 car, 1 frog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakdown into 4 Steps\n",
    "\n",
    "1. What\n",
    "1. When\n",
    "1. How\n",
    "1. Why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Log Likelihood Loss - `NLLLoss`\n",
    "\n",
    "#### What\n",
    "\n",
    "- Same as **Sparse Categorical Cross Entropy** in Tensorflow\n",
    "- The most commonly used loss function for classification problem in neural network\n",
    "\n",
    "#### When\n",
    "\n",
    "- Y is in integer encoding. eg: [0, 1, 2, 3]\n",
    "- Multi-classes\n",
    "- Single label - Each train sample is assigned to only 1 label.\n",
    "- Very common, but not the goto loss function in PyTorch\n",
    "\n",
    "#### How\n",
    "\n",
    "- Y is in integer encoding. It starts from 0.  \n",
    "    eg: Y = [0, 2, 1] coresponding to x1 has Class 0, x2 has Class 2, x3 has Class 1.\n",
    "- Use `log_softmax` function on the output layer\n",
    "- The # of nodes from output score is same as the # of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = torch.FloatTensor([\n",
    "        [3.2, 5.1, -1.7],\n",
    "        [1.3, 4.9, 2],\n",
    "        [2.2, 2.5, -3.1],\n",
    "])\n",
    "y = torch.LongTensor([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = F.log_softmax(score, dim=1)\n",
    "print(s, '\\n')\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "loss = loss_fn(s, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale arbitary values into probability in range [0, 1]\n",
    "s = F.softmax(score, dim=1)\n",
    "print(x, '\\n')\n",
    "\n",
    "# logarithm is a monotonic decrease function. log(x) is negative when 0= < x <= 0.\n",
    "# So we add negative sign to inverse that.\n",
    "s = -torch.log(s)\n",
    "print(x, '\\n')\n",
    "\n",
    "# For each train example, we pick the score based on the index of the label.\n",
    "# And then compute it's mean.\n",
    "loss = (s[0][y[0]] + s[1][y[1]] + s[2][y[2]]) / 3\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properties\n",
    "\n",
    "- The expected inital value is `-log(1/num_classes) == log(num_classes)` All nodes from score has the same probability, 1/num_classes.\n",
    "- Minimal: 0\n",
    "- Maximal: inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss - `CrossEntropyLoss`\n",
    "\n",
    "#### What\n",
    "\n",
    "Combines `LogSoftmax` and `NLLoss` into a single function.\n",
    "\n",
    "#### When\n",
    "\n",
    "- It applies to the same train set as `NLLoss`.\n",
    "- When you don't need to output probability of a prediction.\n",
    "\n",
    "#### How\n",
    "\n",
    "Same as `NLLoss`, but it does NOT require `log_softmax` activation function\n",
    "\n",
    "#### Why\n",
    "\n",
    "- It saves 1 step when computing forward prediction.\n",
    "- It's easier for testing the resistance of your model against adversarial attacks.\n",
    "- There's no extra parameters to train, so `CrossEntropyLoss` and `NLLoss` are interchangeable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(score, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Classification Hinge loss\n",
    "Hinge loss function for SVM\n",
    "\n",
    "#### What\n",
    "\n",
    "#### When\n",
    "\n",
    "#### How \n",
    "\n",
    "#### Why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = torch.FloatTensor([\n",
    "        [3.2, 5.1, -1.7],\n",
    "        [1.3, 4.9, 2],\n",
    "        [2.2, 2.5, -3.1],\n",
    "])\n",
    "y = torch.LongTensor([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MultiMarginLoss(p=1, margin=1.0, reduction='none')\n",
    "loss = loss_fn(score, y)\n",
    "print(loss)\n",
    "\n",
    "loss_fn = nn.MultiMarginLoss(p=1, margin=1.0)\n",
    "loss = loss_fn(score, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_diag = torch.diag(score).unsqueeze(1)\n",
    "print(s_diag, '\\n')\n",
    "\n",
    "s_y = torch.mm(s_diag, torch.ones(1,3))\n",
    "margin = score - s_y + 1\n",
    "margin = margin - torch.eye(3)\n",
    "print(margin, '\\n')\n",
    "\n",
    "z = torch.zeros(3, 3)\n",
    "margin = torch.max(margin, z)\n",
    "print(margin.sum(dim=1), '\\n')\n",
    "\n",
    "loss = margin.sum() / 9\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min\n",
    "y_mat = -torch.ones(3, 3) + 2*torch.eye(3)\n",
    "print(y_mat, '\\n')\n",
    "\n",
    "x = 1e10 * y_mat\n",
    "print(x, '\\n')\n",
    "\n",
    "print(loss_fn(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -1e10 * y_mat\n",
    "print(x, '\\n')\n",
    "\n",
    "print(loss_fn(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class multi-classification hinge loss\n",
    "Use this method when a train example can be signed with multiple labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = torch.FloatTensor([\n",
    "        [3.2, 5.1, -1.7],\n",
    "        [1.3, 4.9, 2],\n",
    "        [2.2, 2.5, -3.1],\n",
    "])\n",
    "y = torch.LongTensor([0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_multi_hinge = torch.LongTensor(range(3)).unsqueeze(dim=1).mm(torch.ones(1, 3, dtype=torch.long))\n",
    "print(y_multi_hinge, '\\n')\n",
    "\n",
    "loss_fn = nn.MultiLabelMarginLoss()\n",
    "loss = loss_fn(score, y_multi_hinge)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_y = torch.mm(torch.diag(score).unsqueeze(1), torch.ones(1,3))\n",
    "print(s_y, '\\n')\n",
    "\n",
    "s_y = torch.mm(s_diag, torch.ones(1,3))\n",
    "margin = score - s_y + 1\n",
    "margin = margin - torch.eye(3)\n",
    "print(margin, '\\n')\n",
    "\n",
    "z = torch.zeros(3, 3)\n",
    "margin = torch.max(margin, z)\n",
    "print(margin.sum(dim=1), '\\n')\n",
    "\n",
    "loss = margin.sum() / 3\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy Loss\n",
    "Using this method when the last layer is `Sigmoid` function and labels are using **one-hot encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.sigmoid(score)\n",
    "print(x, '\\n')\n",
    "# Binary Cross Entropy Loss\n",
    "y_oh = torch.eye(3)\n",
    "print(y_oh, '\\n')\n",
    "loss_fn = nn.BCELoss()\n",
    "loss = loss_fn(x, y_oh)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -(y_oh*torch.log(x) + (1-y_oh)*torch.log(1-x))\n",
    "loss = loss.mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy Loss with Logits\n",
    "Combine the `Sigmoid` layer and the `BCELoss` into one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss = loss_fn(score, y_oh)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin Loss\n",
    "- It's similar to hinge loss but with smooth curve.\n",
    "- Adventage: Differentiable!\n",
    "- y in {-1, 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = torch.FloatTensor([\n",
    "        [3.2, 5.1, -1.7],\n",
    "        [1.3, 4.9, 2],\n",
    "        [2.2, 2.5, -3.1],\n",
    "])\n",
    "y = -torch.ones(3, 3) + 2*torch.eye(3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.SoftMarginLoss()\n",
    "loss = loss_fn(score, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(1 + torch.exp(-y * score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(1 + torch.exp(-y * score)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min\n",
    "x = 1e10 * y\n",
    "print(x, '\\n')\n",
    "loss_fn(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max\n",
    "x = -1e10 * y\n",
    "print(x, '\\n')\n",
    "loss_fn(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Label Soft Margin Loss\n",
    "\n",
    "- Multi-label one-versus-all\n",
    "- y in {0, 1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = torch.FloatTensor([\n",
    "        [3.2, 5.1, -1.7],\n",
    "        [1.3, 4.9, 2],\n",
    "        [2.2, 2.5, -3.1],\n",
    "])\n",
    "y = torch.eye(3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MultiLabelSoftMarginLoss()\n",
    "loss = loss_fn(score, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min\n",
    "x = 1e10 * (-torch.ones(3) + 2*torch.eye(3))\n",
    "print(x, '\\n')\n",
    "loss_fn(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max\n",
    "x = -1e10 * (-torch.ones(3) + 2*torch.eye(3))\n",
    "loss_fn(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python36964bitvenvvenv794a3f6500e74251b078ca195c3ad1e1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
