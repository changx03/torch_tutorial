{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9 (default, Nov  7 2019, 10:44:02) \n",
      "[GCC 8.3.0]\n",
      "/usr/lib/python36.zip\n",
      "/usr/lib/python3.6\n",
      "/usr/lib/python3.6/lib-dynload\n",
      "\n",
      "/home/lukec/venv/lib/python3.6/site-packages\n",
      "/home/lukec/Downloads/jax/build\n",
      "/home/lukec/.local/lib/python3.6/site-packages\n",
      "/usr/local/lib/python3.6/dist-packages\n",
      "/usr/lib/python3/dist-packages\n",
      "/home/lukec/venv/lib/python3.6/site-packages/IPython/extensions\n",
      "/home/lukec/.ipython\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(*sys.path, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for trained CNN\n",
    "root = os.path.join('.', 'dataset_root')\n",
    "# mean, std = [0.13066046], [0.30150425] # based on training set\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "momentum=0.9\n",
    "step_size=6\n",
    "gamma=0.1\n",
    "epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor_grid, mean=0., std=1., title=None):\n",
    "    assert isinstance(tensor_grid, torch.Tensor)\n",
    "    assert len(tensor_grid.size()) == 4, \\\n",
    "        f'For a batch of images only, {tensor_grid.size()} '\n",
    "    \n",
    "    tensor_grid = tv.utils.make_grid(tensor_grid)\n",
    "    grid = tensor_grid.numpy().transpose((1,2,0))\n",
    "    grid = std * grid + mean\n",
    "    grid = np.clip(grid, 0, 1)\n",
    "    plt.imshow(grid)\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "        \n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "# foolbox model expects raw numpy array as image\n",
    "transform = tv.transforms.Compose([\n",
    "        tv.transforms.ToTensor(),\n",
    "#         tv.transforms.Normalize(mean, std)\n",
    "])\n",
    "train_dataset = tv.datasets.MNIST(\n",
    "    root,\n",
    "    train=True, \n",
    "    download=True,\n",
    "    transform=transform)\n",
    "test_dataset = tv.datasets.MNIST(\n",
    "    root,\n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 5**2 * 64)\n",
    "        self.convt1 = nn.ConvTranspose2d(64, 64, 3, 2)\n",
    "        self.convt2 = nn.ConvTranspose2d(64, 32, 3, 2)\n",
    "        self.convt3 = nn.ConvTranspose2d(32, 32, 3, 1)\n",
    "        self.convt4 = nn.ConvTranspose2d(32, 1, 3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        print(x.size())\n",
    "        x = torch.reshape(x, (x.size(0), 64, 5, 5))\n",
    "        x = F.relu(self.convt1(x))\n",
    "        print(x.size())\n",
    "        x = F.relu(self.convt2(x))\n",
    "        print(x.size())\n",
    "        x = F.relu(self.convt3(x))\n",
    "        print(x.size())\n",
    "        x = self.convt4(x)\n",
    "        x = F.interpolate(x, (28, 28))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1600])\n",
      "torch.Size([1, 64, 11, 11])\n",
      "torch.Size([1, 32, 23, 23])\n",
      "torch.Size([1, 32, 25, 25])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,128)\n",
    "decoder = Decoder()\n",
    "output = decoder(x)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "imshow(output.cpu().detach())\n",
    "plt.show()\n",
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    corrects = 0\n",
    "        \n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # for display\n",
    "        total_loss += loss.item() * batch_size\n",
    "        preds = output.max(1, keepdim=True)[1]\n",
    "        corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "    \n",
    "    n = len(train_loader.dataset)\n",
    "    total_loss = total_loss / n\n",
    "    accuracy = corrects / n\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            output = model(x)\n",
    "            loss = F.nll_loss(output, y)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = output.max(1, keepdim=True)[1]\n",
    "            corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "    \n",
    "    n = len(test_loader.dataset)\n",
    "    total_loss = total_loss / n\n",
    "    accuracy = corrects / n\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: NO GPU AT SCHOOL!\n",
    "model1 = Net1()\n",
    "model_linear = Model_linear()\n",
    "model_softmax = torch.nn.Sequential(\n",
    "    model1,\n",
    "    model_linear,\n",
    "    torch.nn.LogSoftmax(dim=1)\n",
    ")\n",
    "model_softmax.to(device)\n",
    "optimizer = torch.optim.SGD(model_softmax.parameters(), lr=lr, momentum=momentum)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, \n",
    "    step_size=step_size, \n",
    "    gamma=gamma)\n",
    "\n",
    "print(model_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    best_model_state = copy.deepcopy(model_softmax.state_dict())\n",
    "    best_tr_acc = 0.0\n",
    "    best_va_acc = 0.0\n",
    "    prev_loss = 1e10\n",
    "    \n",
    "    tr_loss, tr_acc = train(model_softmax, train_loader, optimizer)\n",
    "    va_loss, va_acc = validate(model_softmax, test_loader)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # save best result\n",
    "    if tr_acc >= best_tr_acc and va_acc >= best_va_acc:\n",
    "        best_model_state = copy.deepcopy(model_softmax.state_dict())\n",
    "        best_tr_acc = tr_acc\n",
    "        best_va_acc = va_acc\n",
    "    \n",
    "    # display\n",
    "    time_elapsed = time.time() - start\n",
    "    print(('[{:2d}] {:.0f}m {:.1f}s Train Loss: {:.4f} Accuracy: {:.4f}%, ' +\n",
    "        'Test Loss: {:.4f} Accuracy: {:.4f}%').format(\n",
    "            epoch+1, time_elapsed // 60, time_elapsed % 60,\n",
    "            tr_loss, tr_acc*100.,\n",
    "            va_loss, va_acc*100.))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {:.0f}m {:.1f}s'.format(\n",
    "    time_elapsed // 60,\n",
    "    time_elapsed % 60))\n",
    "print(f'Best val Acc: {best_va_acc:4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_softmax.state_dict(), 'mnist_model3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n",
    "unpool = nn.MaxUnpool2d(2, stride=2)\n",
    "input_ = torch.ones((1,1,4,4))\n",
    "output, indices = pool(input_)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpool(output, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
