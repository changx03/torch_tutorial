{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9 (default, Nov  7 2019, 10:44:02) \n",
      "[GCC 8.3.0]\n",
      "/home/xcha011/venvtorch/lib/python36.zip\n",
      "/home/xcha011/venvtorch/lib/python3.6\n",
      "/home/xcha011/venvtorch/lib/python3.6/lib-dynload\n",
      "/usr/lib/python3.6\n",
      "\n",
      "/home/xcha011/venvtorch/lib/python3.6/site-packages\n",
      "/home/xcha011/.local/lib/python3.6/site-packages\n",
      "/usr/lib/python3.6/site-packages\n",
      "/usr/local/lib/python3.6/dist-packages\n",
      "/usr/lib/python3/dist-packages\n",
      "/home/xcha011/venvtorch/lib/python3.6/site-packages/IPython/extensions\n",
      "/home/xcha011/.ipython\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(*sys.path, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for trained CNN\n",
    "root = os.path.join('.', 'dataset_root')\n",
    "model_path = os.path.join('.', 'mnist_simple_full.pt')\n",
    "mean, std = [0.13066046], [0.30150425] # based on training set\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor_grid, mean=0., std=1., title=None):\n",
    "    assert isinstance(tensor_grid, torch.Tensor)\n",
    "    assert len(tensor_grid.size()) == 4, \\\n",
    "        f'For a batch of images only, {tensor_grid.size()} '\n",
    "    \n",
    "    tensor_grid = tv.utils.make_grid(tensor_grid)\n",
    "    grid = tensor_grid.numpy().transpose((1,2,0))\n",
    "    grid = std * grid + mean\n",
    "    grid = np.clip(grid, 0, 1)\n",
    "    plt.imshow(grid)\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "        \n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "# foolbox model expects raw numpy array as image\n",
    "transform = tv.transforms.Compose([\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "test_dataset = tv.datasets.MNIST(\n",
    "    root,\n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(int(((28 - (3-1)) / 2)**2 * 8), 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            output = model(x)\n",
    "            loss = F.nll_loss(output, y)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = output.max(1, keepdim=True)[1]\n",
    "            corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "    \n",
    "    n = len(test_loader.dataset)\n",
    "    total_loss = total_loss / n\n",
    "    accuracy = corrects / n\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: NO GPU AT SCHOOL!\n",
    "model = Net()\n",
    "model_softmax = torch.nn.Sequential(\n",
    "    model,\n",
    "    torch.nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(test_loader.dataset))\n",
    "# va_loss, va_acc = validate(model, test_loader)\n",
    "# print('Test Loss: {:.4f} Accuracy: {:.4f}%'.format(va_loss, va_acc*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Net(\n",
       "    (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (fc1): Linear(in_features=1352, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       "  (1): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained parameters\n",
    "# this will load both models\n",
    "model_softmax.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "model_softmax.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(test_loader.dataset))\n",
    "# va_loss, va_acc = validate(model_softmax, test_loader)\n",
    "# print('Test Loss: {:.4f} Accuracy: {:.4f}%'.format(va_loss, va_acc*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same accuracy as softmax model, this proves the model has trained parameters\n",
    "# The loss function is way off, which is expected.\n",
    "# print(len(test_loader.dataset))\n",
    "# va_loss, va_acc = validate(model, test_loader)\n",
    "# print('Test Loss: {:.4f} Accuracy: {:.4f}%'.format(va_loss, va_acc*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 1 28 28\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "batch_size, c, h, w = images.size()\n",
    "print(batch_size, c, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6, 7, 4, 3, 3, 3, 2, 0\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=[16, 6])\n",
    "imshow(images, title='MNIST')\n",
    "plt.show()\n",
    "print(*labels.numpy(), sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arctanh(x, epsilon=1e-6):\n",
    "    '''\n",
    "    formular:\n",
    "    https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#Inverse_hyperbolic_tangent\n",
    "    '''\n",
    "    assert isinstance(x, torch.Tensor), f'{type(x)} is not a torch.Tensor!'\n",
    "\n",
    "    x = x * (1-epsilon)  # to enhance numeric stability. avoiding devide by zero\n",
    "    return 0.5 * torch.log((1.+x) / (1.-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tanh_sapce(image, bound=[-.5, .5]):\n",
    "    box_mul = (bound[1] - bound[0]) * .5\n",
    "    box_plus = (bound[1] + bound[0]) * .5\n",
    "    return arctanh(image - box_plus) / box_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_tanh_space(w, bound=[-.5, .5]):\n",
    "    box_mul = (bound[1] - bound[0]) * .5\n",
    "    box_plus = (bound[1] + bound[0]) * .5\n",
    "    return torch.tanh(w)*box_mul + box_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels, num_classes):\n",
    "    assert isinstance(labels, torch.Tensor), \\\n",
    "        f'{type(labels)} is not a torch.Tensor!'\n",
    "    assert labels.max().item() < num_classes, \\\n",
    "        f'Invalid label {labels.max()} > {num_classes}'\n",
    "    \n",
    "    labels_t = labels.unsqueeze(1)\n",
    "    y_onehot = torch.zeros(len(labels), num_classes, dtype=torch.int8)\n",
    "    return y_onehot.scatter_(1, labels_t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for C&W L2 attack\n",
    "BINARY_SEARCH_STEPS = 9  # number of times to adjust the constant with binary search\n",
    "MAX_ITERATIONS = 10000   # number of iterations to perform gradient descent\n",
    "ABORT_EARLY = True       # if we stop improving, abort gradient descent early\n",
    "LEARNING_RATE = 1e-2     # larger values converge faster to less accurate results\n",
    "TARGETED = True          # should we target one specific class? or just be wrong?\n",
    "CONFIDENCE = 0           # how strong the adversarial example should be\n",
    "INITIAL_C_MULTIPLIER = 1e-3     # the initial constant c_multiplier to pick as a first guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets size: torch.Size([8]), dtype: torch.int64 , grid: False\n"
     ]
    }
   ],
   "source": [
    "# We want all adversarial examples to be classified as 3\n",
    "targets = torch.ones(batch_size, dtype=torch.long) * 3\n",
    "print('targets size: {}, dtype: {} , grid: {}'.format(\n",
    "    targets.size(),\n",
    "    targets.dtype,\n",
    "    targets.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For constant c\n",
    "# set the lower and upper bounds accordingly\n",
    "# lower_bound <= c_multiplier <= upper_bound\n",
    "lower_bound_np = np.zeros(batch_size, dtype=np.float32)           # lower bound for c\n",
    "c_multiplier_np = np.ones(batch_size, dtype=np.float32) * INITIAL_C_MULTIPLIER  # current c\n",
    "upper_bound_np = np.ones(batch_size, dtype=np.float32) * 1e10     # upper bound for c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# overall score\n",
    "o_best_l2 = np.ones(batch_size, dtype=np.float32) * np.inf  # overall least L2 norms. Set to infinity at start\n",
    "o_best_l2_ppred = -np.ones(batch_size, dtype=np.float32)    # the perturbed predictions with the least L2 norms. Set to -1, since no class is matched.\n",
    "o_best_advx = torch.zeros_like(images)\n",
    "print(o_best_advx.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_tanh size: torch.Size([8, 1, 28, 28]), dtype: torch.float32 , grid: False\n"
     ]
    }
   ],
   "source": [
    "inputs_tanh = to_tanh_sapce(images)\n",
    "print('inputs_tanh size: {}, dtype: {} , grid: {}'.format(\n",
    "    inputs_tanh.size(), inputs_tanh.dtype, inputs_tanh.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets size: torch.Size([8]), dtype: torch.int64 , grid: False\n",
      "tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "targets_onehot = one_hot_encoding(targets, num_classes=NUM_CLASSES)\n",
    "print('targets size: {}, dtype: {} , grid: {}'.format(\n",
    "    targets.size(), targets.dtype, targets.requires_grad))\n",
    "print(targets_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pert_tanh size: torch.Size([8, 1, 28, 28]), dtype: torch.float32 , grid: True\n"
     ]
    }
   ],
   "source": [
    "# the perturbation variable to optimize\n",
    "# `pert_tanh` is the adversarial examples in tanh-space, it is refered as w_i in the paper\n",
    "pert_tanh = torch.zeros_like(images, requires_grad=True)\n",
    "print('pert_tanh size: {}, dtype: {} , grid: {}'.format(\n",
    "    pert_tanh.size(), pert_tanh.dtype, pert_tanh.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam with learning rate 0.01 is used in the paper\n",
    "optimizer = torch.optim.Adam([pert_tanh], lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010])\n",
      "c: tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010])\n",
      "c: tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010])\n",
      "c: tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010])\n",
      "c: tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010])\n",
      "c: tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010])\n",
      "c: tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010])\n",
      "c: tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010])\n",
      "c: tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010])\n"
     ]
    }
   ],
   "source": [
    "# `repeat` guarantees at least attempt the largest `c_multiplier` once.\n",
    "# the larger `c_multiplier` becomes, the easier to find adversarial.\n",
    "# If `c_multiplier` = 0, there will be no perturbation, and x' = x.\n",
    "binary_search_steps = BINARY_SEARCH_STEPS\n",
    "repeat = binary_search_steps >= 10\n",
    "\n",
    "# search for constant\n",
    "for sstep in range(BINARY_SEARCH_STEPS):\n",
    "    if repeat and sstep == binary_search_steps - 1:\n",
    "        c_multiplier_np = upper_bound_np\n",
    "    \n",
    "    # TODO: inplace or clone?\n",
    "    c_multiplier = torch.from_numpy(np.copy(c_multiplier_np))\n",
    "    print(f'c: {c_multiplier}')\n",
    "    \n",
    "    best_l2 = np.ones(batch_size) * np.inf  # least L2 norm in current epoch\n",
    "    best_l2_ppred = -np.ones(batch_size)\n",
    "    \n",
    "    # previous (summed) batch loss, to be used in early stopping policy\n",
    "    prev_batch_loss = np.inf  # type: float\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
