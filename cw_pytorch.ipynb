{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9 (default, Nov  7 2019, 10:44:02) \n",
      "[GCC 8.3.0]\n",
      "/usr/lib/python36.zip\n",
      "/usr/lib/python3.6\n",
      "/usr/lib/python3.6/lib-dynload\n",
      "\n",
      "/home/lukec/venv/lib/python3.6/site-packages\n",
      "/home/lukec/Downloads/jax/build\n",
      "/home/lukec/.local/lib/python3.6/site-packages\n",
      "/usr/local/lib/python3.6/dist-packages\n",
      "/usr/lib/python3/dist-packages\n",
      "/home/lukec/venv/lib/python3.6/site-packages/IPython/extensions\n",
      "/home/lukec/.ipython\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(*sys.path, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'  # use CPU for testing. GPU doesn't have inf and nan. It could overflow.\n",
    "print(device)\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for trained CNN\n",
    "root = os.path.join('.', 'dataset_root')\n",
    "model_path = os.path.join('.', 'mnist_simple_full.pt')\n",
    "# mean, std = [0.13066046], [0.30150425] # based on training set\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor_grid, mean=0., std=1., title=None):\n",
    "    assert isinstance(tensor_grid, torch.Tensor)\n",
    "    assert len(tensor_grid.size()) == 4, \\\n",
    "        f'For a batch of images only, {tensor_grid.size()} '\n",
    "    \n",
    "    tensor_grid = tv.utils.make_grid(tensor_grid)\n",
    "    grid = tensor_grid.numpy().transpose((1,2,0))\n",
    "    grid = std * grid + mean\n",
    "    grid = np.clip(grid, 0, 1)\n",
    "    plt.imshow(grid)\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "        \n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "# foolbox model expects raw numpy array as image\n",
    "# NOTE: the normalize function put range to [-.4, 2.8]! NO!\n",
    "# https://developers.google.com/machine-learning/data-prep/transform/normalization\n",
    "transform = tv.transforms.Compose([\n",
    "        tv.transforms.ToTensor(),\n",
    "#         tv.transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "test_dataset = tv.datasets.MNIST(\n",
    "    root,\n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(int(((28 - (3-1)) / 2)**2 * 8), 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            output = model(x)\n",
    "            loss = F.nll_loss(output, y)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = output.max(1, keepdim=True)[1]\n",
    "            corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "    \n",
    "    n = len(test_loader.dataset)\n",
    "    total_loss = total_loss / n\n",
    "    accuracy = corrects / n\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: NO GPU AT SCHOOL!\n",
    "model = Net()\n",
    "model_softmax = torch.nn.Sequential(\n",
    "    model,\n",
    "    torch.nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Net(\n",
       "    (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (fc1): Linear(in_features=1352, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       "  (1): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained parameters\n",
    "# this will load both models\n",
    "model_softmax.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model_softmax.to(device)\n",
    "model.eval()\n",
    "model_softmax.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Test Loss: 0.2478 Accuracy: 92.8300%\n"
     ]
    }
   ],
   "source": [
    "print(len(test_loader.dataset))\n",
    "va_loss, va_acc = validate(model_softmax, test_loader)\n",
    "print('Test Loss: {:.4f} Accuracy: {:.4f}%'.format(va_loss, va_acc*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Test Loss: -9.2143 Accuracy: 92.8300%\n"
     ]
    }
   ],
   "source": [
    "# Same accuracy as softmax model, this proves the model has trained parameters\n",
    "# The loss function is way off, which is expected.\n",
    "print(len(test_loader.dataset))\n",
    "va_loss, va_acc = validate(model, test_loader)\n",
    "print('Test Loss: {:.4f} Accuracy: {:.4f}%'.format(va_loss, va_acc*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 1 28 28\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "batch_size, c, h, w = images.size()\n",
    "print(batch_size, c, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 7, 2, 7, 5, 0, 3, 3\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=[16, 6])\n",
    "imshow(images.cpu().detach(), title='MNIST')\n",
    "plt.show()\n",
    "print(*labels.cpu().detach().numpy(), sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for C&W L2 attack\n",
    "BINARY_SEARCH_STEPS = 9           # number of times to adjust the constant with binary search\n",
    "MAX_ITERATIONS = 10000            # number of iterations to perform gradient descent\n",
    "ABORT_EARLY = True                # if we stop improving, abort gradient descent early\n",
    "LEARNING_RATE = 1e-2              # larger values converge faster to less accurate results\n",
    "TARGETED = True                   # should we target one specific class? or just be wrong?\n",
    "CONFIDENCE = 0                    # how strong the adversarial example should be\n",
    "INITIAL_C_MULTIPLIER = 1e-3       # the initial constant c_multiplier to pick as a first guess\n",
    "# NOTE: the normalize function does NOT scale the range to -1, 1.\n",
    "BOX_BOUND = [0., 1.]              # [-1., 1.] is used in original paper\n",
    "# NOTE: np.inf will return nan when np.inf*0. This is a problem!\n",
    "INF = 1e10                        # np.inf does NOT play well with pytorch. 1e10 was used in carlini's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arctanh(x, epsilon=1e-6):\n",
    "    '''\n",
    "    formular:\n",
    "    https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#Inverse_hyperbolic_tangent\n",
    "    '''\n",
    "    assert isinstance(x, torch.Tensor), f'{type(x)} is not a torch.Tensor!'\n",
    "\n",
    "    x = x * (1-epsilon)  # to enhance numeric stability. avoiding devide by zero\n",
    "    return 0.5 * torch.log((1.+x) / (1.-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tanh_sapce(image, bound=BOX_BOUND):\n",
    "    box_mul = (bound[1] - bound[0]) * .5\n",
    "    box_plus = (bound[1] + bound[0]) * .5\n",
    "    return arctanh((image - box_plus) / box_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_tanh_space(w, bound=BOX_BOUND):\n",
    "    box_mul = (bound[1] - bound[0]) * .5\n",
    "    box_plus = (bound[1] + bound[0]) * .5\n",
    "    return torch.tanh(w)*box_mul + box_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1410) tensor(1.) tensor(0.) tensor(0.1410) tensor(0.3197)\n"
     ]
    }
   ],
   "source": [
    "images = next(iter(test_loader))[0]\n",
    "print(images.mean(), images.max(), images.min(), images.mean(), images.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.5241, 10.4418,  0.8034,  2.3046, -2.3201, -2.4535, -1.8035, -0.3795,\n",
      "          2.8484, -0.7358]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-8.5241, 10.4418,  0.8034,  2.3046, -2.3201, -2.4535, -1.8035, -0.3795,\n",
      "          2.8484, -0.7358]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# sanity check: Can tanh transform get back to original value?\n",
    "x = images[:1]\n",
    "# print(x.mean(), x.max(), x.min(), x.mean(), x.std())\n",
    "outputs = model(x)\n",
    "print(outputs)\n",
    "tran_x = from_tanh_space(to_tanh_sapce(x)) \n",
    "# print(tran_x.mean(), tran_x.max(), tran_x.min(), tran_x.mean(), tran_x.std())\n",
    "outputs = model(tran_x)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels, num_classes):\n",
    "    assert isinstance(labels, torch.Tensor), \\\n",
    "        f'{type(labels)} is not a torch.Tensor!'\n",
    "    assert labels.max().item() < num_classes, \\\n",
    "        f'Invalid label {labels.max()} > {num_classes}'\n",
    "    \n",
    "    labels_t = labels.unsqueeze(1)\n",
    "    y_onehot = torch.zeros(len(labels), num_classes, dtype=torch.int8)\n",
    "    return y_onehot.scatter_(1, labels_t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l2_norm(x, y):\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    assert isinstance(y, torch.Tensor)\n",
    "    assert x.size() == y.size()\n",
    "    \n",
    "    b = x.size(0)\n",
    "    return torch.sum(torch.pow(x - y, 2).view(b, -1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets size: torch.Size([8]), dtype: torch.int64 , grid: False\n"
     ]
    }
   ],
   "source": [
    "# We want all adversarial examples to be classified as 3\n",
    "targets = torch.ones(batch_size, dtype=torch.long) * 3\n",
    "print('targets size: {}, dtype: {} , grid: {}'.format(\n",
    "    targets.size(),\n",
    "    targets.dtype,\n",
    "    targets.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For constant c\n",
    "# set the lower and upper bounds accordingly\n",
    "# lower_bound <= c_multiplier <= upper_bound\n",
    "lower_bound_np = np.zeros(batch_size, dtype=np.float32)           # lower bound for c\n",
    "c_multiplier_np = np.ones(batch_size, dtype=np.float32) * INITIAL_C_MULTIPLIER  # current c\n",
    "upper_bound_np = np.ones(batch_size, dtype=np.float32) * 1e10     # upper bound for c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# overall score\n",
    "o_best_l2 = np.ones(batch_size, dtype=np.float32) * INF  # overall least L2 norms. Set to infinity at start\n",
    "o_best_l2_ppred = -np.ones(batch_size, dtype=np.float32)    # the perturbed predictions with the least L2 norms. Set to -1, since no class is matched.\n",
    "o_best_advx = torch.zeros_like(images)\n",
    "print(o_best_advx.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_tanh size: torch.Size([8, 1, 28, 28]), dtype: torch.float32 , grid: False\n"
     ]
    }
   ],
   "source": [
    "inputs_tanh = to_tanh_sapce(images)\n",
    "print('inputs_tanh size: {}, dtype: {} , grid: {}'.format(\n",
    "    inputs_tanh.size(), inputs_tanh.dtype, inputs_tanh.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets size: torch.Size([8]), dtype: torch.int64 , grid: False\n",
      "tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "targets_onehot = one_hot_encoding(targets, num_classes=NUM_CLASSES)\n",
    "print('targets size: {}, dtype: {} , grid: {}'.format(\n",
    "    targets.size(), targets.dtype, targets.requires_grad))\n",
    "print(targets_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pert_tanh size: torch.Size([8, 1, 28, 28]), dtype: torch.float32 , grid: True\n"
     ]
    }
   ],
   "source": [
    "# the perturbation variable to optimize\n",
    "# `pert_tanh` is the adversarial examples in tanh-space, it is refered as w_i in the paper\n",
    "pert_tanh = torch.zeros_like(images, requires_grad=True)\n",
    "print('pert_tanh size: {}, dtype: {} , grid: {}'.format(\n",
    "    pert_tanh.size(), pert_tanh.dtype, pert_tanh.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam with learning rate 0.01 is used in the paper\n",
    "optimizer = torch.optim.Adam([pert_tanh], lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(model, optimizer, inputs_tanh, pert_tanh, targets_oh, c):\n",
    "    # NOTE: Do we optimize tanh(w) or tanh(input+pert)?\n",
    "    advxs = from_tanh_space(inputs_tanh + pert_tanh)  # the trained model does not known tanh-space\n",
    "    pert_outputs = model(advxs)  # predictions on adversarial examples\n",
    "    \n",
    "    inputs = from_tanh_space(inputs_tanh)\n",
    "    perts_norm = get_l2_norm(inputs, advxs)  # the L2 distance is measured in image-sapce\n",
    "    \n",
    "#     print(pert_outputs)\n",
    "    max_other_activ = torch.max(((1-targets_oh)*pert_outputs - targets_oh*1e4), 1)[0]\n",
    "    print(max_other_activ)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010])\n",
      "tensor([10.4418,  7.1559,  7.2195,  6.2449,  8.6826, 11.4414, 11.7757, 10.7949],\n",
      "       grad_fn=<MaxBackward0>)\n",
      "tensor([10.4418,  7.1559,  7.2195,  6.2449,  8.6826, 11.4414, 11.7757, 10.7949],\n",
      "       grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# `repeat` guarantees at least attempt the largest `c_multiplier` once.\n",
    "# the larger `c_multiplier` becomes, the easier to find adversarial.\n",
    "# If `c_multiplier` = 0, there will be no perturbation, and x' = x.\n",
    "binary_search_steps = BINARY_SEARCH_STEPS\n",
    "repeat = binary_search_steps >= 10\n",
    "\n",
    "# search for constant\n",
    "# for sstep in range(BINARY_SEARCH_STEPS):\n",
    "for sstep in range(1):\n",
    "    if repeat and sstep == binary_search_steps - 1:\n",
    "        c_multiplier_np = upper_bound_np\n",
    "    \n",
    "    # TODO: inplace or clone?\n",
    "    c_multiplier = torch.from_numpy(np.copy(c_multiplier_np))\n",
    "    print(f'c: {c_multiplier}')\n",
    "    \n",
    "    best_l2 = np.ones(batch_size) * INF  # least L2 norm in current epoch\n",
    "    best_l2_ppred = -np.ones(batch_size)\n",
    "    \n",
    "    # previous (summed) batch loss, to be used in early stopping policy\n",
    "    prev_batch_loss = INF  # type: float\n",
    "    \n",
    "    # optimization step\n",
    "#     for ostep in range(MAX_ITERATIONS):\n",
    "    for ostep in range(2):\n",
    "        optimize(\n",
    "            model, optimizer, \n",
    "            inputs_tanh, pert_tanh, targets_onehot, \n",
    "            c_multiplier)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
