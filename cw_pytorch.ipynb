{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9 (default, Nov  7 2019, 10:44:02) \n",
      "[GCC 8.3.0]\n",
      "/usr/lib/python36.zip\n",
      "/usr/lib/python3.6\n",
      "/usr/lib/python3.6/lib-dynload\n",
      "\n",
      "/home/lukec/venv/lib/python3.6/site-packages\n",
      "/home/lukec/Downloads/jax/build\n",
      "/home/lukec/.local/lib/python3.6/site-packages\n",
      "/usr/local/lib/python3.6/dist-packages\n",
      "/usr/lib/python3/dist-packages\n",
      "/home/lukec/venv/lib/python3.6/site-packages/IPython/extensions\n",
      "/home/lukec/.ipython\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version)\n",
    "print(*sys.path, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'  # use CPU for testing. GPU doesn't have inf and nan. It could overflow.\n",
    "print(device)\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for trained CNN\n",
    "root = os.path.join('.', 'dataset_root')\n",
    "model_path = os.path.join('.', 'mnist_simple_full.pt')\n",
    "# mean, std = [0.13066046], [0.30150425] # based on training set\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor_grid, mean=0., std=1., title=None):\n",
    "    assert isinstance(tensor_grid, torch.Tensor)\n",
    "    assert len(tensor_grid.size()) == 4, \\\n",
    "        f'For a batch of images only, {tensor_grid.size()} '\n",
    "    \n",
    "    tensor_grid = tv.utils.make_grid(tensor_grid)\n",
    "    grid = tensor_grid.numpy().transpose((1,2,0))\n",
    "    grid = std * grid + mean\n",
    "    grid = np.clip(grid, 0, 1)\n",
    "    plt.imshow(grid)\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "        \n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "# foolbox model expects raw numpy array as image\n",
    "# NOTE: the normalize function put range to [-.4, 2.8]! NO!\n",
    "# https://developers.google.com/machine-learning/data-prep/transform/normalization\n",
    "transform = tv.transforms.Compose([\n",
    "        tv.transforms.ToTensor(),\n",
    "#         tv.transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "test_dataset = tv.datasets.MNIST(\n",
    "    root,\n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(int(((28 - (3-1)) / 2)**2 * 8), 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            output = model(x)\n",
    "            loss = F.nll_loss(output, y)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = output.max(1, keepdim=True)[1]\n",
    "            corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "    \n",
    "    n = len(test_loader.dataset)\n",
    "    total_loss = total_loss / n\n",
    "    accuracy = corrects / n\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: NO GPU AT SCHOOL!\n",
    "model = Net()\n",
    "model_softmax = torch.nn.Sequential(\n",
    "    model,\n",
    "    torch.nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Net(\n",
       "    (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (fc1): Linear(in_features=1352, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       "  (1): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained parameters\n",
    "# this will load both models\n",
    "model_softmax.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model_softmax.to(device)\n",
    "model.eval()\n",
    "model_softmax.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Test Loss: 0.2478 Accuracy: 92.8300%\n"
     ]
    }
   ],
   "source": [
    "print(len(test_loader.dataset))\n",
    "va_loss, va_acc = validate(model_softmax, test_loader)\n",
    "print('Test Loss: {:.4f} Accuracy: {:.4f}%'.format(va_loss, va_acc*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Test Loss: -9.2143 Accuracy: 92.8300%\n"
     ]
    }
   ],
   "source": [
    "# Same accuracy as softmax model, this proves the model has trained parameters\n",
    "# The loss function is way off, which is expected.\n",
    "print(len(test_loader.dataset))\n",
    "va_loss, va_acc = validate(model, test_loader)\n",
    "print('Test Loss: {:.4f} Accuracy: {:.4f}%'.format(va_loss, va_acc*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 1 28 28\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "batch_size, c, h, w = images.size()\n",
    "print(batch_size, c, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4],\n",
      "        [9],\n",
      "        [7],\n",
      "        [8],\n",
      "        [7],\n",
      "        [0],\n",
      "        [3],\n",
      "        [7]])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(images)\n",
    "preds = outputs.max(1, keepdim=True)[1]\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA54AAAClCAYAAAA02iIUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZRUxfn/8c8juBBFEY2IS2IUCIvhh8pXiF+jooKoOWIQF1QW9Sf+jEuiuKBx+SlBFJWvxoUIiuCSoIBRjMoicQuGhEWNCkdRhLAKiiZIlADW949pr1XFdE/PTN+e6Z736xwP9XT1vbdg7nR32fXUY845AQAAAACQlm3qegAAAAAAgPLGxBMAAAAAkComngAAAACAVDHxBAAAAACkioknAAAAACBVTDwBAAAAAKli4gkAAAAASBUTTwAAKmFmS8zsP2a2e/T4G2bmzGw/MxuXaR/q9bcyM+fFL5vZ//Xia83sIzP7wsyWm9kTmcffzTz2hZltMbOvvPjaYvydAQBICxNPAACy+0hS328CM/uRpO9Ez1kn6df5nMzMBkjqJ+lY59xOkjpLmilJzrkOzrmdMo+/Junib2Ln3C21/6sAAFB3mHgCAJDdo5L6e/EASY9EzxkvqaOZHZnH+f5L0jTn3IeS5Jxb7ZwbXZCRAgBQjzHxBAAgu9mSdjazdmbWSNIZkh6LnvNvSbdIGpbn+fqb2ZVm1jlzTgAAyh4TTwAAcvvmW8/ukhZKWlHJcx6Q9D0zOz7XiZxzj0m6RNJxkl6RtMbMri7scAEAqH+YeAIAkNujks6UNFBbL7OVJDnnNkoamvkvJ+fc4865YyU1k/T/JA01s+MKNloAAOohJp4AAOTgnFuqik2GTpD0VI6nPqyKyWTvPM+7yTk3UdLfJR1Y23ECAFCfNa7rAQAAUALOk7Src26DmVX63umc22xmN0r6TbaTmNlASWslvSppgyqW3HaQ9NeCjxgAgHqEbzwBAKiCc+5D59zcPJ76e0mrcvT/S9K1kv4h6XNJIyRd6Jz7c+1HCQBA/WXOuaqfBQAAAABADfGNJwAAAAAgVUw8AQAAAACpYuIJAAAAAEhVrSaeZtbTzN4zsw/MbEihBgUAAAAAKB813lzIzBpJel9Sd0nLJc2R1Nc5t6BwwwMAAAAAlLra1PE8VNIHzrnFkmRmEyT1kpR14mlmbKELAAAAAOXrE+fcd+MHa7PUdm9Jy7x4eeYxAAAAAEDDtLSyB2vzjWdezGyQpEFpXwcAAAAAUD/VZuK5QtK+XrxP5rGAc260pNESS20BAAAAoCGqzVLbOZJam9kPzGw7SWdImlKYYQEAAAAAykWNv/F0zm02s4slTZPUSNJY59y7BRsZAAAAAKAs1LicSo0uxlJbAAAAAChn85xzneMHa7PUFgAAAACAKjHxBAAAAACkioknAAAAACBVTDwBAAAAAKli4gkAAAAASBUTTwAAAABAqph4AgAAAABSxcQTAAAAAJAqJp4AAAAAgFQx8QQAAAAApIqJJwAAAAAgVUw8AQAAAACpYuIJAAAAAEgVE08AAAAAQKoa1/UAAAAAgFI2ZsyYID733HODeJttvv2u5+uvvw76Jk2aFMRTp04N4hdeeCHrsWvWrKn+YIE6wjeeAAAAAIBUMfEEAAAAAKSKiScAAAAAIFXmnCvexcyKd7Eia9w4TJdt2bJl3scuW7as0MMBAABl7sQTT0zaK1euDPreeOONVK558MEHB3GXLl2yPnf27NlFGVN9MH369CA++uijg9jMknZtPntv3LgxiEeNGhXEjz76aNZj165dG8TxPQMU0DznXOf4Qb7xBAAAAACkioknAAAAACBVTDwBAAAAAKkix7NARo8eHcRx/aZc4vyIefPmFWRMqL3TTjstiLt37x7Effr0SdrNmjUL+s4+++wgfvzxxws8OqC4Bg8eHMSnn3560u7cOUzl8POZJOnvf/970r7sssuCvj/96U+FGiJQ1uLPFnfeeWfS3rBhQ9Dn539K0ltvvZX1vPG+FAcccEAQDxw4MGkff/zxQV+LFi2ynnf16tVB/MgjjwTxtddem/XYUhP/G957771BfPLJJyft2nz2jl9bq3OupUuXBvF5552XtF9++eUajwmoBDmeAAAAAIDiY+IJAAAAAEgVS21roW/fvkn7jjvuCPripSfLly9P2vH24iNGjAji+fPnF2qIqES3bt2CuGPHjkF8zjnnJO3WrVsHfU2aNAniXL8/S5YsyXndTz75JGnH26Nv2bIl63nLzQ477BDE8RIv33HHHRfE+++/f42v26NHj6zXnDJlShD7y6bj5WzlZtttt03aY8aMCfrif/+HHnooaf/2t78N+rp27RrEt956a9J+7bXXgj7/dw6l7fDDD0/aAwYMCPo++uijIL7llluKMqZSFr8+vvfee0G89957Zz02fg864YQTgtgv5RYvszzkkEOqMcr8xWOaPHly0r7hhhuCvvh9sdRdcsklSTv+7NC/f/8gXrBgQRD7aT3V+RxSFf+zRu/evYO+5557rsbnre+23377IP7FL34RxP59Gv87VOczwI477hjETZs2DWJ/2Xq7du1ynuvCCy9M2nPnzg364t+d+D22jrDUFgAAAABQfEw8AQAAAACpqnLiaWZjzWyNmb3jPdbczGaY2aLMn7umO0wAAAAAQKlqnMdzxkm6V5K/B/YQSTOdc7ea2ZBMfHXhh1e/7bnnnpW2pa3X3H/66adJ+/zzzw/61q9fn8Loqs7RKOeyLbvvvnsQ33XXXUn71FNPDfr8nDYp/Nn5ubmSNGTIkKzXPProo4O4Z8+eQXzMMccE8f3335+0x40bF/RdfPHFQVxOOZ9XXHFFEMc/j5rmFtVmi/n4uUcddVQQf//730/ace5NufF/H+K8ozgHJc43802aNCmIW7VqlbTjcir4VuPG374t77PPPkFf27Ztg3i33XZL2rNmzarxNf2SClKYp1nd/LE2bdok7bi8xJdffhnEfq7UfffdF/Rt3ry5WtctJ35e51NPPRX05crpjO23335BHOexderUKWnH5ZDS2v8jHpNfomnx4sVB3wMPPJDKGOrKPffck7Uv/rvHubBXXnll0o73EHnzzTdrPCb/9WbXXRvOd0jDhw8P4ksvvTTrc994440g9ksYSdLChQuDuF+/fkn7yCOPDPoOPvjgIK7p79kRRxwRxHfffXfO69QnVX7j6Zx7VdK66OFeksZn2uMlnSwAAAAAACqRzzeelWnhnFuVaa+WlLV6sJkNkjSohtcBAAAAAJS4mk48E845l6tMinNutKTRUvmVUwEAAAAAVK2mE8+Pzaylc26VmbWUtKaQg6qvzj333CD2a3dus024avnrr78OYn+NeCFzOv31+ZJ03XXXJe3rr78+6Pviiy+CeJdddinYOOragQceGMRDhw4N4l69emU99vPPPw9iv7ZcnHvp196M+fXIpK1rP/k1D2Nxvbs4t6U2ORz1wS9/+cukPWzYsKAvvoeLWVs4mzi3t9zzOn1+PvGqVauCvrVr19b4vI8//njSjus3xjm1cU3BhsTP77799tvzPq6q96Dq8M9Vm/PE4tp5/nvo008/HfQtXbq0YNctNX6+pV9vuLrefvvtIL7gggtqfC7/M0z8vhjXPa7OmHbaaacaj6mUxXtLxPUd/ZxOKXztjV8v4/0NqsN/v/3qq69qfJ5S0759+7yfG+dLPvbYY4UeTq3FNc39Mc+fP7/Yw8mppuVUpkj65pPyAEnPFGY4AAAAAIByk085ld9L+oukH5rZcjM7T9Ktkrqb2SJJx2ZiAAAAAAC2UuVSW+dc3yxdx2R5HAAAAACARK03Fyo3fo2puLZZnB/hr42P82DiHKWrr06nzKmf0xnHcT5i7969UxlDXfHzOqdNmxb0xXVVfU888UQQxzUz/ZqrVfHzNOIczrhuZ5y7+O9//ztpn3nmmUFfqed0xn784x8n7UaNGuV93Ouvvx7EmzZtyvrczz77LIjjmoGHHXZYEPu1OfGtjRs3Ju1jjz026PPv2ULiZ/Etvw5mKYhz5P37J67jicp16NAhiH/3u9/lfaxfSzfOC4z3ecgl3gPi8ssvD+IXXnghaa9cuTLv88Zy5Qw3b968xuctNXGOoZ8DX5Xa1KyO8/QvueSSpB3XXi5n8b4N8XtdWuI82okTJybtuHZrvG/IzJkzk7Zfw1na+ndy0aJFtRlmqmqa4wkAAAAAQF6YeAIAAAAAUtXgl9rGy5r87d1POOGEvM/z8MMPB3G8tLY6yzd98XLNq666Kojj5QH+8trhw4cHfbNmzarRGOqrgw46KGnnWlorSbNnz07aAwcODPr8pWGxeIvzbt26BbFf9mSPPfbIOYZ4iYtfKuHZZ5/NeWypGzx4cNK+6aab8j7u/fffD+LNmzfXeAz+khYp9/LOeDl2Q7Vw4cKCnSsupYHK+a/xLVq0CPri96t169Yl7bFjxwZ98VKs2267rUbjiVM24ve21atXB/EZZ5yRtOMyUajcrrvuGsR77bVX3sf6y2vjZawdO3bM+zynnnpqEE+fPj3vYwvlkEMOKfo1G5pXXnkliBvS8lrfNddcE8TxZ7CePXsW5DqjRo0K4g0bNgRxdUqU5fr8Ey/hLWTZxkLjG08AAAAAQKqYeAIAAAAAUsXEEwAAAACQqgaX4/nTn/40iMePHx/EzZo1S9pVbVE9ZsyYpH3hhRcWYHRbu/nmm4O4S5cuOZ8/YcKEpH3XXXelMqa6EudQjhgxIu9jly9fnrR79eqV87l+2Y3u3bsHfe3atQviXPdIXM4jzhmeP39+znGUE//fv64cccQRQRxvSe975pln0h5Og3P66adn7Yu3kW/I/LIWp5xySsHOu3jx4qT9wx/+MOiL9yB48MEHa3ydvn2/Lf29zTb8v+18+HtLVGXFihVB7JezifeAiPOq49I3fg5uWjmdnTp1CmK/BFlsxowZqYyhPor3Lxg2bFgQ59qDoEmTJkFcndeJPn36BLFfyufdd9/N+zylLt7b46WXXsoZ14V+/foFsZ/zH7+2vvrqq0UZUyHwrgAAAAAASBUTTwAAAABAqph4AgAAAABS1SByPH/0ox8l7UGDBgV9fk6nlDvv66233griOJ+ipho3Dn8Mf/jDH5J2165dcx4b1+2aN29e0o7zO3LVqywFO+ywQxDH/265+DkQcT5E/DOvKrc3X3F+bkPK6awPjjrqqCDeeeedg9j/OX/44YdB39tvv53auBqq3XffPWn/5S9/Cfri2nIovKeffroo1/F/r+L3pzj291j4+OOP0x1YPfKzn/0siNu3b5/3sc8991wQ+/nz8R4WcU7nBRdckPNcheLnosX3XZyf6Ncf9OvSlrt4D4gbb7wx72MbNWoUxG3btg3iKVOmJO1cuaKS9OKLLybtuG78ggUL8h4TCu/4448P4lyvrXPnzi3KmAqBbzwBAAAAAKli4gkAAAAASFVZLrWNl5iOHDkyaXfr1i3oy7WsMu5r3rx5EPvLYP0t8CvzzjvvJO3169cHfdddd10Q+1+vV7XsM94C3d9+uVWrVkHfDTfckPNc9d0//vGPIJ4zZ07S7tmzZ43P+8EHHwTxli1bknZcdiC2adOmpH377bcHfeVWzqbUfPe73w3iXNv4Uz6l8Nq0aRPEZ599dtIePnx4sYeDIvF/l/xyHZXZb7/9knacSuEvwSw3e+65ZxB/5zvfyfvY+HfnzDPPzPrc559/PognT56c93Vqw/8Mts8+++R87qJFi5L2xIkTUxtTOfE/o0hbl0Hxl8zGnxEPOOCAIPbvxZ/85CdBH0tti+uQQw4J4rgEn++zzz4L4pkzZ6YypjTwjScAAAAAIFVMPAEAAAAAqWLiCQAAAABIVVnmeN5zzz1BHOd11lScq/DCCy8k7apKcvhr8ON80LhkSnXKecS5jf6xccmCcuNvHb/NNrn/H4qfNxuvhY+3G7/pppuSdpyntmHDhiD2y9dcf/31VYwYxdSjR4+8nztu3Lj0BtJA3XvvvUG82267Je2zzjor6DvxxBOD+J///GcQjxgxImnPnj27UENECg4//PC8n3vkkUcm7V122SXoi0uBlJP7778/iOPSCL5bb701iJctWxbE/r93/Dnk4osvrukQqyUe45VXXpn3sblK2JWCpk2bJu39998/6Fu7dm0Qr1y5sihjWrJkSdK+/PLLg75JkyYFsb/3wbBhw4K+WbNmBbG/VwkK77LLLgvinXbaKetzx4wZE8SrVq1KZUxp4BtPAAAAAECqmHgCAAAAAFLFxBMAAAAAkKqyzPE877zzgrg6OZNp6dChQ1Gu46/zfuONN4pyzbri58XkypGRpGeffTZp+7lmkjRlypQgjvM0fHPnzg3io48+uspxojjiurWnn356zuf7+Tbr1q1LZUwNWZzrt3Tp0qQd1yCL87ziOoevv/560l6+fHnQ1759+yCuqqYy0hW/vmJr8ftVdeqJx/y8PH8vg0Jr3Pjbj4vxvhT9+/cP4lxjXrNmTRBfddVVBRhd8fg5nZL08MMPJ+2TTz456PNf86Sta2gWwx//+Mcgjj8XHnrooUm7WbNmQV/Lli2DmBzPwvNrHffu3Tvv46ZOnZrGcIqCbzwBAAAAAKli4gkAAAAASBUTTwAAAABAqsoyx9PP55OkI444ImmvX78+6HvwwQeD2K/TGOcZ5coZi3OUmjdvHsRNmjTJ+9hc4lo9fs1JaevaPtiaX39Vyp138eKLLwZxdWpDori23377IN5xxx1zPn/s2LFJe/Xq1amMqSFr3bp1EPu5l3GdzphfW06S9tprr6Qd54T9+c9/DuLzzz8/ac+ZMye/waJg/Pezquoro/ZWrFiRtDdu3Fiw8/o5nVJYD/KWW26p8Xnjz2fTp0+v8bnqQrwHRJzX6YtrhPv5fOPHjy/swPI0efLkIPZzPGP9+vUL4hkzZqQypoYknlf4NW/jzzCxk046KWm/8sorhR1YEfGuAAAAAABIVZUTTzPb18xeMrMFZvaumf0i83hzM5thZosyf+6a/nABAAAAAKUmn6W2myUNds7NN7OmkuaZ2QxJAyXNdM7damZDJA2RdHV6Q81fvPShTZs2Sfv999+v8Xmrs+33E088EcSnnHJK3se+/PLLSXvUqFFB33PPPRfEX375Zd7nbUjiJV5DhgxJ2gcddFDOY7ds2ZK0J06cWNiBITXxkumqyhCwNXy6/CWA1bVp06Yg9ssSXHTRRUHfr371qyD2l8d37Ngx63mQDv/3Li4bUlXZK1TfsmXLkvbdd98d9B133HFBXJ33sxtuuCGIr7322ryP9ZfSx59hfv3rX+d9nvpo7dq1Qey/psRLa2N+uZu6WmpbHUuWLKnrIZSdOH2rbdu2STv+zDJr1qwgnjlzZnoDK6Iqv/F0zq1yzs3PtNdLWihpb0m9JH3zmzNeUvaF7gAAAACABqtamwuZ2X6SDpL0V0ktnHPf7HSzWlKLLMcMkjSo5kMEAAAAAJSyvDcXMrOdJE2W9Evn3L/8Plfx/XCl69qcc6Odc52dc51rNVIAAAAAQEnK6xtPM9tWFZPOx51zT2Ue/tjMWjrnVplZS0lr0hpkbdUmr7MurunnREyaNKkQw2lwunbtGsRDhw7N+tx4Xf3IkSOTNuVpSkec6xebOnVqEMfb+qM0DRs2LIj79OmTtH/+858HfVdfXS+2IUADF+eXd+jQIetzW7VqFcRxaTZ/n4cRI0YEfXEe4d/+9rcgPvDAA5P2fffdF/S1aFHpIrZKxbmld9xxR9KOS8CVupUrVwbxMccck7Tj0jDxvgN+Sb5//Sv4/kaDBqWzMDC+X7p16xbE/n4YcQ72a6+9lsqYGpJTTz01iNu1axfE/ufPTz/9NOi78cYbg/irr74q8OjqRj672pqkhyQtdM6N9LqmSPqmKNEASc8UfngAAAAAgFKXzzee/y2pn6S3zezNzGPXSrpV0pNmdp6kpZJOS2eIAAAAAIBSVuXE0zn3Z0mWpfuYLI8DAAAAACCpmrvaIrumTZsG8ZFHHpn3sXFtzjhG1fw8C2nr3NiKFeOV83PCpK3zNFB/+XXqOncO9y+Lc3fj/KaNGzemNzDUmSeffDJpxzky5ZozU6r8OoEN6Wfh50BK0gMPPBDE2223XdKO35922223IM5Vy7tHjx5B/MwzYUaUn+MZv0fGr59+/t9DDz0U9F133XV5j6nc+Pdwz549g765c+cG8S677JK0e/fuHfRtv/32QfzII48E8SeffJK0N2/eHPStX78+iDt16pS0r7jiiqDvsMMOC2L/5xr/3OLzovrieyKXxx57LIhffvnlAo+mfsh7V1sAAAAAAGqCiScAAAAAIFVMPAEAAAAAqSLHs0CaNWsWxHvssUcQ58oxjOsJNqT8iNro2LFj0r7llluCvp133jmI/XyVWbNmBX3xOvovvviiQCNE2nr16pX3cydMmJDiSFBf+PndcY3PXK/DKL5HH300aX/88cd1OJLi8v/e0ta1OnPVJI7rMFaHn9NZXf6YL7zwwhqfp5wtXrw4iKdNmxbEp532bfGH+DPjWWedlTP2bdq0KYjj+o977rln1YPN+M9//pO0L7300qBv9uzZeZ8H3/Jrd/br1y/nc/1au9dcc01qY6pP+MYTAAAAAJAqJp4AAAAAgFSx1LZAWrduHcTx0pl4a3LfqlWrUhlTuTnooIOC2C87U9XSkuuvvz5pjxw5MuhjaXPp6NKlSxD7S5e22Sb8/2jz5s0L4mXLlqU3MNQb69atq+shNGj+cub4dxKVGzp0aBDff//9Sfv222/PeexRRx2VtPfaa68aj2HRokVBHKcmxMvWUbW+ffsG8fz585P28OHDa3xev9yOVL2ltbELLrggacclXJCfuBSOX7arUaNGQd+GDRuC+M4770za/rLncsa7AgAAAAAgVUw8AQAAAACpYuIJAAAAAEgVOZ4F0rhx+E/pb+lflTgXDRXif9N77rkniHPlNcRb848aNSppk9NZui6//PIg9rek//rrr4O+qVOnBjE/94bhpJNOqushNGj+fgbx72Qco8LmzZuD2H//6t+/f85j27dvn7QHDBgQ9E2fPj2Iv/rqq6znWb16dRB/+OGHOa+L6vP3l1i7dm3Q17NnzyDu06dPja/jf/5cunRp0Od/FpKkJUuW1Pg6DVWTJk2C+De/+U0Qt23bNmnH+7uce+65QbxgwYICj67+4xtPAAAAAECqmHgCAAAAAFLFxBMAAAAAkCpyPAskzqWIY1SfX59Mkg477LCsz43zFM4///wgprZfadpxxx2D2M9nim3ZsiWIn3/++VTGhPolrmk3ePDgpH3bbbcFfRs3bizKmIBi8XPErr766jocCariv0eNGzcu6Itj1F/du3cP4nPOOSfvY+Oc24aIbzwBAAAAAKli4gkAAAAASBVLbVFvvfPOO0E8bdq0IO7Ro0fSfumll4K+mTNnpjcwFE3r1q2DuF27dlmfG29p/vrrr6cyJtStfffdN4hvvvnmID7ggAOS9oQJE4I+ynmk78orr0zaXbp0CfratGlT7OEAQK3E5VOuuOKKvI+Ny6XMmTOnIGMqZXzjCQAAAABIFRNPAAAAAECqmHgCAAAAAFJlzrniXcyseBcDUPK+973vBfH48eOD2M8DnjhxYtD36quvpjcwAFUaOHBgEA8fPjyIe/bsmbTfeuutYgwJAKqladOmQRzvHxHvPeHndR577LFB35o1awo8unptnnOuc/wg33gCAAAAAFLFxBMAAAAAkComngAAAACAVJHjCQAAAAAoFHI8AQAAAADFV+XE08x2MLO/mdlbZvaumd2UefwHZvZXM/vAzJ4ws+3SHy4AAAAAoNTk843nRklHO+f+j6ROknqaWVdJt0n6H+dcK0mfSTovvWECAAAAAEpVlRNPV+GLTLht5j8n6WhJkzKPj5d0ciojBAAAAACUtLxyPM2skZm9KWmNpBmSPpT0uXNuc+YpyyXtneXYQWY218zmFmLAAAAAAIDSktfE0zm3xTnXSdI+kg6V1DbfCzjnRjvnOle2sxEAAAAAoPxVa1db59znkl6S9GNJzcyscaZrH0krCjw2AAAAAEAZyGdX2++aWbNMu4mk7pIWqmIC2ifztAGSnklrkAAAAACA0tW46qeopaTxZtZIFRPVJ51zfzSzBZImmNmvJb0h6aEUxwkAAAAAKFHmnCvexcyKdzEAAAAAQLHNq2x/n3y+8SykTyQtlbR7pg2UI+5vlDvucZQz7m+UO+5xpO37lT1Y1G88k4uazWWXW5Qr7m+UO+5xlDPub5Q77nHUlWrtagsAAAAAQHUx8QQAAAAApKquJp6j6+i6QDFwf6PccY+jnHF/o9xxj6NO1EmOJwAAAACg4WCpLQAAAAAgVUWdeJpZTzN7z8w+MLMhxbw2kBYzW2Jmb5vZm2Y2N/NYczObYWaLMn/uWtfjBPJhZmPNbI2ZveM9Vun9bBV+k3lN/7uZHVx3Iwfyk+Ue//9mtiLzOv6mmZ3g9V2TucffM7Pj6mbUQH7MbF8ze8nMFpjZu2b2i8zjvI6jzhVt4mlmjSTdJ+l4Se0l9TWz9sW6PpCybs65Tt725EMkzXTOtZY0MxMDpWCcpJ7RY9nu5+Mltc78N0jSqCKNEaiNcdr6Hpek/8m8jndyzj0vSZnPKWdI6pA55v7M5xmgvtosabBzrr2krpIuytzHvI6jzhXzG89DJX3gnFvsnPuPpAmSehXx+kAx9ZI0PtMeL+nkOhwLkDfn3KuS1kUPZ7ufe0l6xFWYLamZmbUszkiBmslyj2fTS9IE59xG59xHkj5QxecZoF5yzq1yzs3PtNdLWihpb/E6jnqgmBPPvSUt8+LlmceAUuckTTezeWY2KPNYC+fcqkx7taQWdTM0oCCy3c+8rqOcXJxZajjWS4/gHkfJMrP9JB0k6a/idRz1AJsLAbV3uHPuYFUsV7nIzI7wO13F1tFsH42ywP2MMjVK0gGSOklaJenOuh0OUDtmtpOkyZJ+6Zz7l9/H6zjqSjEnnisk7evF+2QeA0qac25F5s81kv6gimVYH3+zVCXz55q6GyFQa9nuZ17XURaccx8757Y451vyvZAAAAFtSURBVL6WNEbfLqflHkfJMbNtVTHpfNw591TmYV7HUeeKOfGcI6m1mf3AzLZTRbL+lCJeHyg4M9vRzJp+05bUQ9I7qri3B2SeNkDSM3UzQqAgst3PUyT1z+yK2FXSP72lXEDJiHLafqaK13Gp4h4/w8y2N7MfqGIDlr8Ve3xAvszMJD0kaaFzbqTXxes46lzjYl3IObfZzC6WNE1SI0ljnXPvFuv6QEpaSPpDxeu8Gkv6nXNuqpnNkfSkmZ0naamk0+pwjEDezOz3ko6StLuZLZd0o6RbVfn9/LykE1Sx4cq/JZ1T9AED1ZTlHj/KzDqpYvnhEkkXSJJz7l0ze1LSAlXsFnqRc25LXYwbyNN/S+on6W0zezPz2LXidRz1gFUs8wYAAAAAIB1sLgQAAAAASBUTTwAAAABAqph4AgAAAABSxcQTAAAAAJAqJp4AAAAAgFQx8QQAAAAApIqJJwAAAAAgVUw8AQAAAACp+l9R2QzX04Hq0QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4, 9, 7, 3, 7, 0, 3, 7\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=[16, 6])\n",
    "imshow(images.cpu().detach(), title='MNIST')\n",
    "plt.show()\n",
    "print(*labels.cpu().detach().numpy(), sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1427) tensor(1.) tensor(0.) tensor(0.1427) tensor(0.3207)\n"
     ]
    }
   ],
   "source": [
    "print(images.mean(), images.max(), images.min(), images.mean(), images.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for C&W L2 attack\n",
    "BINARY_SEARCH_STEPS = 9           # number of times to adjust the constant with binary search\n",
    "MAX_ITERATIONS = 10000            # number of iterations to perform gradient descent\n",
    "ABORT_EARLY = True                # if we stop improving, abort gradient descent early\n",
    "LEARNING_RATE = 1e-2              # larger values converge faster to less accurate results\n",
    "TARGETED = False                  # should we target one specific class? or just be wrong?\n",
    "CONFIDENCE = 0                    # how strong the adversarial example should be, kappa in the paper\n",
    "INITIAL_C_MULTIPLIER = 1e-3       # the initial constant c_multiplier to pick as a first guess\n",
    "# NOTE: the normalize function does NOT scale the range to -1, 1.\n",
    "BOX_BOUND = [0., 1.]              # [-1., 1.] is used in original paper\n",
    "# NOTE: np.inf will return nan when np.inf*0. This is a problem!\n",
    "INF = 1e10                        # np.inf does NOT play well with pytorch. 1e10 was used in carlini's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arctanh(x, epsilon=1e-6):\n",
    "    '''\n",
    "    formular:\n",
    "    https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#Inverse_hyperbolic_tangent\n",
    "    '''\n",
    "    assert isinstance(x, torch.Tensor), f'{type(x)} is not a torch.Tensor!'\n",
    "\n",
    "    x = x * (1-epsilon)  # to enhance numeric stability. avoiding devide by zero\n",
    "    return 0.5 * torch.log((1.+x) / (1.-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tanh_sapce(image, bound=BOX_BOUND):\n",
    "    box_mul = (bound[1] - bound[0]) * .5\n",
    "    box_plus = (bound[1] + bound[0]) * .5\n",
    "    return arctanh((image - box_plus) / box_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_tanh_space(w, bound=BOX_BOUND):\n",
    "    box_mul = (bound[1] - bound[0]) * .5\n",
    "    box_plus = (bound[1] + bound[0]) * .5\n",
    "    return torch.tanh(w)*box_mul + box_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels, num_classes):\n",
    "    assert isinstance(labels, torch.Tensor), \\\n",
    "        f'{type(labels)} is not a torch.Tensor!'\n",
    "    assert labels.max().item() < num_classes, \\\n",
    "        f'Invalid label {labels.max()} > {num_classes}'\n",
    "    \n",
    "    labels_t = labels.unsqueeze(1)\n",
    "    y_onehot = torch.zeros(len(labels), num_classes, dtype=torch.int8)\n",
    "    return y_onehot.scatter_(1, labels_t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l2_norm(x, y):\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    assert isinstance(y, torch.Tensor)\n",
    "    assert x.size() == y.size()\n",
    "    \n",
    "    b = x.size(0)\n",
    "    return torch.sum(torch.pow(x - y, 2).view(b, -1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want all adversarial examples to be classified as 3\n",
    "if TARGETED:\n",
    "    targets = torch.ones(batch_size, dtype=torch.long) * 3\n",
    "    print('targets size: {}, dtype: {} , grid: {}'.format(\n",
    "        targets.size(),\n",
    "        targets.dtype,\n",
    "        targets.requires_grad))\n",
    "else: \n",
    "    targets = labels  # for untargeted attack, targets are same as labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For constant c\n",
    "# set the lower and upper bounds accordingly\n",
    "# lower_bound <= c_multiplier <= upper_bound\n",
    "lower_bound_np = np.zeros(batch_size, dtype=np.float32)           # lower bound for c\n",
    "c_multiplier_np = np.ones(batch_size, dtype=np.float32) * INITIAL_C_MULTIPLIER  # current c\n",
    "upper_bound_np = np.ones(batch_size, dtype=np.float32) * 1e10     # upper bound for c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# overall score\n",
    "o_best_l2 = np.ones(batch_size, dtype=np.float32) * INF  # overall least L2 norms. Set to infinity at start\n",
    "o_best_pred = -np.ones(batch_size, dtype=np.float32)    # the perturbed predictions with the least L2 norms. Set to -1, since no class is matched.\n",
    "o_best_adv = torch.zeros_like(images)\n",
    "print(o_best_adv.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_tanh size: torch.Size([8, 1, 28, 28]), dtype: torch.float32 , grid: False\n"
     ]
    }
   ],
   "source": [
    "inputs_tanh = to_tanh_sapce(images)\n",
    "print('inputs_tanh size: {}, dtype: {} , grid: {}'.format(\n",
    "    inputs_tanh.size(), inputs_tanh.dtype, inputs_tanh.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets size: torch.Size([8]), dtype: torch.int64 , grid: False\n",
      "tensor([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "targets_onehot = one_hot_encoding(targets, num_classes=NUM_CLASSES)\n",
    "print('targets size: {}, dtype: {} , grid: {}'.format(\n",
    "    targets.size(), targets.dtype, targets.requires_grad))\n",
    "print(targets_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pert_tanh size: torch.Size([8, 1, 28, 28]), dtype: torch.float32 , grid: True\n"
     ]
    }
   ],
   "source": [
    "# the perturbation variable to optimize\n",
    "# `pert_tanh` is the adversarial examples in tanh-space, it is refered as w_i in the paper\n",
    "pert_tanh = torch.zeros_like(images, requires_grad=True)\n",
    "print('pert_tanh size: {}, dtype: {} , grid: {}'.format(\n",
    "    pert_tanh.size(), pert_tanh.dtype, pert_tanh.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam with learning rate 0.01 is used in the paper\n",
    "optimizer = torch.optim.Adam([pert_tanh], lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(model, optimizer, inputs_tanh, pert_tanh, targets_oh, c):\n",
    "    # NOTE: `targets_oh` could be 2 different variables\n",
    "    #       1. the true label for original image on untargeted attacks\n",
    "    #       2. the target class for targeted attacks\n",
    "    \n",
    "    # targets_oh (batch, classes): (8, 10)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # adversarial examples\n",
    "    adv_egs = from_tanh_space(inputs_tanh + pert_tanh)  # the trained model does not known tanh-space\n",
    "    pred_outputs = model(adv_egs)  # predictions from adversarial examples BEFORE SOFTMAX\n",
    "    # pred_outputs (batch, classes): (8, 10)\n",
    "    \n",
    "    inputs = from_tanh_space(inputs_tanh)\n",
    "    l2_norms = get_l2_norm(inputs, adv_egs)  # the L2 distance is measured in image-sapce\n",
    "    \n",
    "    # only keeps the output for the target class\n",
    "    target_output = torch.sum(targets_oh*pred_outputs, 1)\n",
    "    # this returns the max output exclude the target label\n",
    "    other_output = torch.max((1-targets_oh)*pred_outputs - targets_oh*INF, 1)[0]\n",
    "    \n",
    "    if TARGETED:\n",
    "        f_loss = torch.clamp(other_output - target_output + CONFIDENCE, min=0.)\n",
    "    else:  # untargeted, the target variable is the true class\n",
    "        f_loss = torch.clamp(target_output - other_output + CONFIDENCE, min=0.)\n",
    "    \n",
    "    # sum up the batch into 1 total loss\n",
    "    # gradient can only be created only for scalar outputs\n",
    "    loss = torch.sum(l2_norms + c*f_loss)\n",
    "    \n",
    "    # optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss, l2_norms, pred_outputs, adv_egs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compensate_confidence(outputs, targets):\n",
    "    assert type(outputs) == np.ndarray\n",
    "    assert type(targets) == np.ndarray\n",
    "    \n",
    "    outputs_comp = np.copy(outputs)\n",
    "    \n",
    "    if TARGETED:\n",
    "        outputs_comp[np.arange(targets.shape[0]), targets] -= CONFIDENCE\n",
    "    else:\n",
    "        outputs_comp[np.arange(targets.shape[0]), targets] += CONFIDENCE\n",
    "    \n",
    "    return outputs_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_attack_success(pred, label):\n",
    "    if TARGETED:  \n",
    "        return pred == label  # match the target label\n",
    "    else:\n",
    "        return pred != label  # anyting other than the ture label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010])\n",
      "Step [0] loss: 0.0342\n",
      "Step [1000] loss: 0.0341\n",
      "Step [2000] loss: 0.0342\n",
      "abort at [0/9] search step [2000/10000] optimize step\n"
     ]
    }
   ],
   "source": [
    "# `repeat` guarantees at least attempt the largest `c_multiplier` once.\n",
    "# the larger `c_multiplier` becomes, the easier to find adversarial.\n",
    "# If `c_multiplier` = 0, there will be no perturbation, and x' = x.\n",
    "binary_search_steps = BINARY_SEARCH_STEPS\n",
    "repeat = binary_search_steps >= 10\n",
    "\n",
    "# search for constant\n",
    "# for sstep in range(BINARY_SEARCH_STEPS):\n",
    "for sstep in range(1):\n",
    "    if repeat and sstep == binary_search_steps - 1:\n",
    "        c_multiplier_np = upper_bound_np\n",
    "    \n",
    "    # TODO: inplace or clone?\n",
    "    c_multiplier = torch.from_numpy(np.copy(c_multiplier_np))\n",
    "    print(f'c: {c_multiplier}')\n",
    "    \n",
    "    best_l2 = np.ones(batch_size) * INF  # least L2 norm in current epoch\n",
    "    best_pred = -np.ones(batch_size)\n",
    "    \n",
    "    # previous (summed) batch loss, to be used in early stopping policy\n",
    "    prev_batch_loss = INF  # type: float\n",
    "    \n",
    "    # optimization step\n",
    "    for ostep in range(MAX_ITERATIONS):\n",
    "        loss, l2_norms, pred_outputs, adv_egs = optimize(\n",
    "            model, optimizer, \n",
    "            inputs_tanh, pert_tanh, targets_onehot, \n",
    "            c_multiplier)\n",
    "        \n",
    "        # print out loss every 10%\n",
    "        if ostep % (MAX_ITERATIONS//10) == 0:\n",
    "            print('Step [{}] loss: {:.4f}'.format(\n",
    "                ostep, loss.cpu().detach().item()))\n",
    "        \n",
    "        # abort early strategy\n",
    "        # check if we should abort search if we're getting nowhere.\n",
    "        if ABORT_EARLY and ostep % (MAX_ITERATIONS//10) == 0: \n",
    "            if loss > prev_batch_loss * (1-1e-4):\n",
    "                print('abort at [{}/{}] search step [{}/{}] optimize step'.format(\n",
    "                    sstep, BINARY_SEARCH_STEPS,\n",
    "                    ostep, MAX_ITERATIONS))\n",
    "                break\n",
    "            prev_batch_loss = loss\n",
    "    \n",
    "        # update result\n",
    "        pred_outputs_np = pred_outputs.cpu().detach().numpy()\n",
    "        targets_np = targets.cpu().detach().numpy()\n",
    "        \n",
    "        max_pred_outputs = np.argmax(pred_outputs_np, axis=1)\n",
    "        comp_pred_outputs = compensate_confidence(pred_outputs_np, targets_np)\n",
    "        comp_pred_outputs = np.argmax(comp_pred_outputs, axis=1)\n",
    "        assert comp_pred_outputs.shape == (BATCH_SIZE,), \\\n",
    "            f'comp_pred_outputs.shape = {comp_pred_outputs.shape}'\n",
    "        \n",
    "        for i in range(BATCH_SIZE):\n",
    "            l2 = l2_norms[i]\n",
    "            pred = comp_pred_outputs[i]\n",
    "            label = targets_np[i]\n",
    "            adv_eg = adv_egs[i]  # a tensor\n",
    "            \n",
    "            if does_attack_success(pred, label):\n",
    "                if l2 < best_l2[i]:\n",
    "                    best_l2[i] = l2\n",
    "                    best_pred[i] = pred\n",
    "                if l2 < o_best_l2[i]:\n",
    "                    o_best_l2[i] = l2\n",
    "                    o_best_pred[i] = pred\n",
    "                    o_best_adv[i] = adv_eg\n",
    "    \n",
    "    # binary search for c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e+10, 1.e+10, 1.e+10, 0.e+00, 1.e+10, 1.e+10, 1.e+10, 1.e+10],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_best_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1.,  8., -1., -1., -1., -1.], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_best_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 28, 28])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_best_adv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4, 9, 7, 3, 7, 0, 3, 7\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=[16, 6])\n",
    "imshow(o_best_adv.cpu().detach())\n",
    "plt.show()\n",
    "print(*labels.cpu().detach().numpy(), sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.066395e-07 -5.364418e-07\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 3\n",
    "adv = np.squeeze(np.transpose(o_best_adv.cpu().detach().numpy()[idx], (1, 2, 0)))\n",
    "image = np.squeeze(np.transpose(images.cpu().detach().numpy()[idx], (1, 2, 0)))\n",
    "p = adv - image\n",
    "imax = np.max(p)\n",
    "imin = np.min(p)\n",
    "print(imax, imin)\n",
    "p = (p - imin)/ (imax-imin)\n",
    "print(p.shape)\n",
    "\n",
    "plt.figure(figsize=[18, 6])\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image, cmap='gray', vmin=0., vmax=1.)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(adv, cmap='gray', vmin=0., vmax=1.)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(p, cmap='gray', vmin=0., vmax=1.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
