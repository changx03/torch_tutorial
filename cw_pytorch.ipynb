{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9 (default, Nov  7 2019, 10:44:02) \n",
      "[GCC 8.3.0]\n",
      "/usr/lib/python36.zip\n",
      "/usr/lib/python3.6\n",
      "/usr/lib/python3.6/lib-dynload\n",
      "\n",
      "/home/lukec/venv/lib/python3.6/site-packages\n",
      "/home/lukec/Downloads/jax/build\n",
      "/home/lukec/.local/lib/python3.6/site-packages\n",
      "/usr/local/lib/python3.6/dist-packages\n",
      "/usr/lib/python3/dist-packages\n",
      "/home/lukec/venv/lib/python3.6/site-packages/IPython/extensions\n",
      "/home/lukec/.ipython\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(*sys.path, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for trained CNN\n",
    "root = os.path.join('.', 'dataset_root')\n",
    "model_path = os.path.join('.', 'mnist_simple_full.pt')\n",
    "mean, std = [0.13066046], [0.30150425] # based on training set\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor_grid, mean=0., std=1., title=None):\n",
    "    assert isinstance(tensor_grid, torch.Tensor)\n",
    "    assert len(tensor_grid.size()) == 4, \\\n",
    "        f'For a batch of images only, {tensor_grid.size()} '\n",
    "    \n",
    "    tensor_grid = tv.utils.make_grid(tensor_grid)\n",
    "    grid = tensor_grid.numpy().transpose((1,2,0))\n",
    "    grid = std * grid + mean\n",
    "    grid = np.clip(grid, 0, 1)\n",
    "    plt.imshow(grid)\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "        \n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "# foolbox model expects raw numpy array as image\n",
    "transform = tv.transforms.Compose([\n",
    "        tv.transforms.ToTensor(),\n",
    "        tv.transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "test_dataset = tv.datasets.MNIST(\n",
    "    root,\n",
    "    train=False, \n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(int(((28 - (3-1)) / 2)**2 * 8), 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            output = model(x)\n",
    "            loss = F.nll_loss(output, y)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = output.max(1, keepdim=True)[1]\n",
    "            corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "    \n",
    "    n = len(test_loader.dataset)\n",
    "    total_loss = total_loss / n\n",
    "    accuracy = corrects / n\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: NO GPU AT SCHOOL!\n",
    "model = Net()\n",
    "model_softmax = torch.nn.Sequential(\n",
    "    model,\n",
    "    torch.nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Net(\n",
       "    (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "    (fc1): Linear(in_features=1352, out_features=128, bias=True)\n",
       "    (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       "  (1): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained parameters\n",
    "# this will load both models\n",
    "model_softmax.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model_softmax.to(device)\n",
    "model.eval()\n",
    "model_softmax.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Test Loss: 0.0608 Accuracy: 98.1500%\n"
     ]
    }
   ],
   "source": [
    "print(len(test_loader.dataset))\n",
    "va_loss, va_acc = validate(model_softmax, test_loader)\n",
    "print('Test Loss: {:.4f} Accuracy: {:.4f}%'.format(va_loss, va_acc*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Test Loss: -15.7470 Accuracy: 98.1500%\n"
     ]
    }
   ],
   "source": [
    "# Same accuracy as softmax model, this proves the model has trained parameters\n",
    "# The loss function is way off, which is expected.\n",
    "print(len(test_loader.dataset))\n",
    "va_loss, va_acc = validate(model, test_loader)\n",
    "print('Test Loss: {:.4f} Accuracy: {:.4f}%'.format(va_loss, va_acc*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 1 28 28\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(test_loader))\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "batch_size, c, h, w = images.size()\n",
    "print(batch_size, c, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3, 5, 2, 7, 8, 5, 8, 3\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=[16, 6])\n",
    "imshow(images.cpu().detach(), title='MNIST')\n",
    "plt.show()\n",
    "print(*labels.cpu().detach().numpy(), sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arctanh(x, epsilon=1e-6):\n",
    "    '''\n",
    "    formular:\n",
    "    https://en.wikipedia.org/wiki/Inverse_hyperbolic_functions#Inverse_hyperbolic_tangent\n",
    "    '''\n",
    "    assert isinstance(x, torch.Tensor), f'{type(x)} is not a torch.Tensor!'\n",
    "\n",
    "    x = x * (1-epsilon)  # to enhance numeric stability. avoiding devide by zero\n",
    "    return 0.5 * torch.log((1.+x) / (1.-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tanh_sapce(image, bound=[-.5, .5]):\n",
    "    box_mul = (bound[1] - bound[0]) * .5\n",
    "    box_plus = (bound[1] + bound[0]) * .5\n",
    "    return arctanh(image - box_plus) / box_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_tanh_space(w, bound=[-.5, .5]):\n",
    "    box_mul = (bound[1] - bound[0]) * .5\n",
    "    box_plus = (bound[1] + bound[0]) * .5\n",
    "    return torch.tanh(w)*box_mul + box_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.5321, -1.9189, -2.6467, 11.0659, -1.8692,  5.6858, -0.2667, -5.2476,\n",
      "         3.5066, -2.2243], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0299,  1.0175,  0.3052, -0.6053,  0.2058,  0.5168, -0.0823,  0.5156,\n",
      "        -1.0810, -0.7532], device='cuda:0', grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-372a26123eae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: tanh-space conversion isn't correct!\n",
    "print(model(images)[0])\n",
    "outputs = model(from_tanh_space(to_tanh_sapce(images)))\n",
    "print(outputs[0])\n",
    "\n",
    "assert False, 'Fix TODO first!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels, num_classes):\n",
    "    assert isinstance(labels, torch.Tensor), \\\n",
    "        f'{type(labels)} is not a torch.Tensor!'\n",
    "    assert labels.max().item() < num_classes, \\\n",
    "        f'Invalid label {labels.max()} > {num_classes}'\n",
    "    \n",
    "    labels_t = labels.unsqueeze(1)\n",
    "    y_onehot = torch.zeros(len(labels), num_classes, dtype=torch.int8)\n",
    "    return y_onehot.scatter_(1, labels_t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l2_norm(x, y):\n",
    "    assert isinstance(x, torch.Tensor)\n",
    "    assert isinstance(y, torch.Tensor)\n",
    "    assert x.size() == y.size()\n",
    "    \n",
    "    b = x.size(0)\n",
    "    return torch.sum(torch.pow(x - y, 2).view(b, -1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for C&W L2 attack\n",
    "BINARY_SEARCH_STEPS = 9           # number of times to adjust the constant with binary search\n",
    "MAX_ITERATIONS = 10000            # number of iterations to perform gradient descent\n",
    "ABORT_EARLY = True                # if we stop improving, abort gradient descent early\n",
    "LEARNING_RATE = 1e-2              # larger values converge faster to less accurate results\n",
    "TARGETED = True                   # should we target one specific class? or just be wrong?\n",
    "CONFIDENCE = 0                    # how strong the adversarial example should be\n",
    "INITIAL_C_MULTIPLIER = 1e-3       # the initial constant c_multiplier to pick as a first guess\n",
    "INF_TORCH = torch.tensor(np.inf)  # a tensor with value infinity\n",
    "\n",
    "assert INF_TORCH > 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want all adversarial examples to be classified as 3\n",
    "targets = torch.ones(batch_size, dtype=torch.long) * 3\n",
    "print('targets size: {}, dtype: {} , grid: {}'.format(\n",
    "    targets.size(),\n",
    "    targets.dtype,\n",
    "    targets.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For constant c\n",
    "# set the lower and upper bounds accordingly\n",
    "# lower_bound <= c_multiplier <= upper_bound\n",
    "lower_bound_np = np.zeros(batch_size, dtype=np.float32)           # lower bound for c\n",
    "c_multiplier_np = np.ones(batch_size, dtype=np.float32) * INITIAL_C_MULTIPLIER  # current c\n",
    "upper_bound_np = np.ones(batch_size, dtype=np.float32) * 1e10     # upper bound for c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall score\n",
    "o_best_l2 = np.ones(batch_size, dtype=np.float32) * np.inf  # overall least L2 norms. Set to infinity at start\n",
    "o_best_l2_ppred = -np.ones(batch_size, dtype=np.float32)    # the perturbed predictions with the least L2 norms. Set to -1, since no class is matched.\n",
    "o_best_advx = torch.zeros_like(images)\n",
    "print(o_best_advx.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_tanh = to_tanh_sapce(images)\n",
    "print('inputs_tanh size: {}, dtype: {} , grid: {}'.format(\n",
    "    inputs_tanh.size(), inputs_tanh.dtype, inputs_tanh.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_onehot = one_hot_encoding(targets, num_classes=NUM_CLASSES)\n",
    "print('targets size: {}, dtype: {} , grid: {}'.format(\n",
    "    targets.size(), targets.dtype, targets.requires_grad))\n",
    "print(targets_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the perturbation variable to optimize\n",
    "# `pert_tanh` is the adversarial examples in tanh-space, it is refered as w_i in the paper\n",
    "pert_tanh = torch.zeros_like(images, requires_grad=True)\n",
    "print('pert_tanh size: {}, dtype: {} , grid: {}'.format(\n",
    "    pert_tanh.size(), pert_tanh.dtype, pert_tanh.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam with learning rate 0.01 is used in the paper\n",
    "optimizer = torch.optim.Adam([pert_tanh], lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(model, optimizer, inputs_tanh, pert_tanh, targets_oh, c):\n",
    "    # NOTE: Do we optimize tanh(w) or tanh(input+pert)?\n",
    "    advxs = from_tanh_space(inputs_tanh + pert_tanh)  # the trained model does not known tanh-space\n",
    "    pert_outputs = model(advxs)\n",
    "    print(pert_outputs)\n",
    "    inputs = from_tanh_space(inputs_tanh)\n",
    "    \n",
    "    perts_norm = get_l2_norm(inputs, advxs)\n",
    "    \n",
    "#     print(pert_outputs)\n",
    "    max_other_activ = torch.max(((1-targets_oh) * pert_outputs - targets_oh * 1e4), 1)[0]\n",
    "    print(max_other_activ)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `repeat` guarantees at least attempt the largest `c_multiplier` once.\n",
    "# the larger `c_multiplier` becomes, the easier to find adversarial.\n",
    "# If `c_multiplier` = 0, there will be no perturbation, and x' = x.\n",
    "binary_search_steps = BINARY_SEARCH_STEPS\n",
    "repeat = binary_search_steps >= 10\n",
    "\n",
    "# search for constant\n",
    "# for sstep in range(BINARY_SEARCH_STEPS):\n",
    "for sstep in range(1):\n",
    "    if repeat and sstep == binary_search_steps - 1:\n",
    "        c_multiplier_np = upper_bound_np\n",
    "    \n",
    "    # TODO: inplace or clone?\n",
    "    c_multiplier = torch.from_numpy(np.copy(c_multiplier_np))\n",
    "    print(f'c: {c_multiplier}')\n",
    "    \n",
    "    best_l2 = np.ones(batch_size) * np.inf  # least L2 norm in current epoch\n",
    "    best_l2_ppred = -np.ones(batch_size)\n",
    "    \n",
    "    # previous (summed) batch loss, to be used in early stopping policy\n",
    "    prev_batch_loss = np.inf  # type: float\n",
    "    \n",
    "    # optimization step\n",
    "#     for ostep in range(MAX_ITERATIONS):\n",
    "    for ostep in range(2):\n",
    "        optimize(\n",
    "            model, optimizer, \n",
    "            inputs_tanh, pert_tanh, targets_onehot, \n",
    "            c_multiplier)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_l2_norm(images, images-1) >= INF_TORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1, -1, 0]) * INF_TORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1, -1, 0]) * np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
